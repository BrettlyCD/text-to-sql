{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Schema Data to Vector Database\n",
    "\n",
    "My initial thought was to build a model that checks for similarity between the prompt and the schema information. But in doing research it sounds like this could be simplified and expedited using a vector database through langchain. We could then query the tables (with metadata) based on the question and return the documents that are most closely related. With this, we'll try to all be bundled into langchain. woohoo!\n",
    "\n",
    "Big shoutout to this great blogpost that provided some of the framework: https://canvasapp.com/blog/text-to-sql-in-production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "I'll test the structure of the data - breaking it out into a way to load into \"documents\"\n",
    "\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in json\n",
    "path = '../data/interim/'\n",
    "\n",
    "with open(path+'schema_info.json', \"r\") as f:\n",
    "    schema_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'custid'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_info[0]['columns'][0]['c_name']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to try to create a \"document\" for each table with metadata of the schema, table, and a list of columns. I don't know if the column type would be helpful to the model, so I may leave just to try, but also don't want to add useless details to storage. I'll leave them out, but comment out the raw code that will work to pull in all the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written_by {'schema': 'imdb', 'table': 'written_by', 'columns': '[\"id\", \"msid\", \"wid\"]'}\n"
     ]
    }
   ],
   "source": [
    "for table in schema_info:\n",
    "    doc = table['table']\n",
    "    cols = table['columns']\n",
    "    col_names = json.dumps([column['c_name'] for column in cols]) #add in json dumps as Chroma didn't take the list -> hoping this converts to a string that works with Chroma\n",
    "    metadata={\n",
    "        'schema': table['schema'],\n",
    "        'table': table['table'],\n",
    "        'columns': col_names,\n",
    "    }\n",
    "\n",
    "print(doc, metadata)\n",
    "\n",
    "\n",
    "###code to return full column_info in metadata\n",
    "#for table in schema_info:\n",
    "#    doc = table['table']\n",
    "#    metadata={\n",
    "#        'schema': table['schema'],\n",
    "#        'table': table['table'],\n",
    "#        'columns': json.dumps(table['columns']),\n",
    "#    }\n",
    "\n",
    "#print(doc, metadata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup langchain Document\n",
    "\n",
    "Documents are just a piece of text that you can optionalliy add metadata too. Straighforward, but they allow for using model and databases with langchain. They have a module for setting up documents: \"Document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(\n",
    "        page_content=f\"Table: {simple_meta['table']}\",\n",
    "        metadata={\n",
    "            'schema': simple_meta['schema'],\n",
    "            'table': simple_meta['table'],\n",
    "            'columns': json.dumps([col['c_name'] for col in simple_meta['columns']])\n",
    "        },\n",
    "    )\n",
    "    for simple_meta in schema_info\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Table: ACCOUNTS', metadata={'schema': 'small_bank_1', 'table': 'ACCOUNTS', 'columns': '[\"custid\", \"name\"]'}),\n",
       " Document(page_content='Table: AREA_CODE_STATE', metadata={'schema': 'voter_1', 'table': 'AREA_CODE_STATE', 'columns': '[\"area_code\", \"state\"]'}),\n",
       " Document(page_content='Table: Acceptance', metadata={'schema': 'workshop_paper', 'table': 'Acceptance', 'columns': '[\"Submission_ID\", \"Workshop_ID\", \"Result\"]'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Vector Database\n",
    "\n",
    "There are a few options for databases, but I'll go with Chroma because it is open source, makes local device use \"easy\", and has built-in connections with langchain.\n",
    "\n",
    "A key component in running apps using langchain is the ability store and work with embeddings, which is how AI models natively represent data of all kinds. Langchain will provide the application framework and Chroma will provide the vector store.\n",
    "\n",
    "Within that we can also do some information retrieval from the database, finding the most relevant tables based on the user question. This will allow us to engineer a better prompt to feed to our gpt chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load .env file to get API Key\n",
    "load_dotenv()\n",
    "openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "#setup embeddings using OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "#setup directory to store database on disk\n",
    "persist_dir = '../data/processed/chromadb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(OpenAIEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(\n",
    "    documents, ebedding=embeddings,\n",
    "    persist_directory=persist_dir\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently getting an error here, I think because I haven't passed it a true working API key yet. I'll keep doing some research on it because I don't want to run it without fully understanding it and rack up cost for no reason."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(path):\n",
    "    \"\"\"Return json file from specified filepath\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        json_file = json.load(f)\n",
    "    \n",
    "    return json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_chroma_documents(json_path):\n",
    "    \"\"\"Take json file and work through it to prepare for load to Chroma.\n",
    "    Instead of list comprehension, establis the blank list and loop through the json.\n",
    "    Then saves to the list using the langchain docstore.document -> Document modeul\n",
    "\n",
    "    This works specifically with the content and metadata we want for this project\"\"\"\n",
    "    docs = []\n",
    "    for item in get_json(json_path):\n",
    "        doc = Document(\n",
    "            page_content=f\"Table: {item['table']}\",\n",
    "            metadata={\n",
    "                'schema': item['schema'],\n",
    "                'table': item['table'],\n",
    "                'columns': json.dumps([col['c_name'] for col in item['columns']])\n",
    "            }\n",
    "        )\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Table: ACCOUNTS', metadata={'schema': 'small_bank_1', 'table': 'ACCOUNTS', 'columns': '[\"custid\", \"name\"]'}),\n",
       " Document(page_content='Table: AREA_CODE_STATE', metadata={'schema': 'voter_1', 'table': 'AREA_CODE_STATE', 'columns': '[\"area_code\", \"state\"]'}),\n",
       " Document(page_content='Table: Acceptance', metadata={'schema': 'workshop_paper', 'table': 'Acceptance', 'columns': '[\"Submission_ID\", \"Workshop_ID\", \"Result\"]'})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = prep_chroma_documents('../data/interim/schema_info.json')\n",
    "\n",
    "test[:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

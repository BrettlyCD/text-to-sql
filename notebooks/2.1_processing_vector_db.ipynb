{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Schema Data to Vector Database\n",
    "\n",
    "My initial thought was to build a model that checks for similarity between the prompt and the schema information. But in doing research it sounds like this could be simplified and expedited using a vector database through langchain. We could then query the tables (with metadata) based on the question and return the documents that are most closely related. With this, we'll try to all be bundled into langchain. woohoo!\n",
    "\n",
    "Big shoutout to this great blogpost that provided some of the framework: https://canvasapp.com/blog/text-to-sql-in-production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "I'll test the structure of the data - breaking it out into a way to load into \"documents\"\n",
    "\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in json\n",
    "path = '../data/interim/'\n",
    "\n",
    "with open(path+'schema_info.json', \"r\") as f:\n",
    "    schema_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'custid'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_info[0]['columns'][0]['c_name']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to try to create a \"document\" for each table with metadata of the schema, table, and a list of columns. I don't know if the column type would be helpful to the model, so I may leave just to try, but also don't want to add useless details to storage. I'll leave them out, but comment out the raw code that will work to pull in all the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written_by {'schema': 'imdb', 'table': 'written_by', 'columns': '[\"id\", \"msid\", \"wid\"]'}\n"
     ]
    }
   ],
   "source": [
    "for table in schema_info:\n",
    "    doc = table['table']\n",
    "    cols = table['columns']\n",
    "    col_names = json.dumps([column['c_name'] for column in cols]) #add in json dumps as Chroma didn't take the list -> hoping this converts to a string that works with Chroma\n",
    "    metadata={\n",
    "        'schema': table['schema'],\n",
    "        'table': table['table'],\n",
    "        'columns': col_names,\n",
    "    }\n",
    "\n",
    "print(doc, metadata)\n",
    "\n",
    "\n",
    "###code to return full column_info in metadata\n",
    "#for table in schema_info:\n",
    "#    doc = table['table']\n",
    "#    metadata={\n",
    "#        'schema': table['schema'],\n",
    "#        'table': table['table'],\n",
    "#        'columns': json.dumps(table['columns']),\n",
    "#    }\n",
    "\n",
    "#print(doc, metadata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup langchain Document\n",
    "\n",
    "Documents are just a piece of text that you can optionalliy add metadata too. Straighforward, but they allow for using model and databases with langchain. They have a module for setting up documents: \"Document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(\n",
    "        page_content=f\"Table: {simple_meta['table']}\",\n",
    "        metadata={\n",
    "            'schema': simple_meta['schema'],\n",
    "            'table': simple_meta['table'],\n",
    "            'columns': json.dumps([col['c_name'] for col in simple_meta['columns']])\n",
    "        },\n",
    "    )\n",
    "    for simple_meta in schema_info\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Table: ACCOUNTS', metadata={'schema': 'small_bank_1', 'table': 'ACCOUNTS', 'columns': '[\"custid\", \"name\"]'}),\n",
       " Document(page_content='Table: AREA_CODE_STATE', metadata={'schema': 'voter_1', 'table': 'AREA_CODE_STATE', 'columns': '[\"area_code\", \"state\"]'}),\n",
       " Document(page_content='Table: Acceptance', metadata={'schema': 'workshop_paper', 'table': 'Acceptance', 'columns': '[\"Submission_ID\", \"Workshop_ID\", \"Result\"]'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Vector Database\n",
    "\n",
    "There are a few options for databases, but I'll go with Chroma because it is open source, makes local device use \"easy\", and has built-in connections with langchain.\n",
    "\n",
    "A key component in running apps using langchain is the ability store and work with embeddings, which is how AI models natively represent data of all kinds. Langchain will provide the application framework and Chroma will provide the vector store.\n",
    "\n",
    "Within that we can also do some information retrieval from the database, finding the most relevant tables based on the user question. This will allow us to engineer a better prompt to feed to our gpt chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d05604ab030442c8ccf1e026fc6f647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)a8e1d/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61cb1d12aa642f99803de3859a43035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21568da18b964e74bb30737da6f6e762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)b20bca8e1d/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8599eff99b4e40c0923de9e01f722c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)0bca8e1d/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df7af3b8c8843029f65825a09e7b731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6070a6b0c034a37b1772834d0b19d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e1d/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f988debe7edc448bafbc3c64657c87b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88c0439a4444ad38f88ea402eaa15e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b44a61e592e427495012eed7a4ed289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5531591252243f08fc5ede82be56585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)a8e1d/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57db795db2dc4167af65ed3154d24698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b9f9f3cfff4c64aff2c5d38c09b180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)8e1d/train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f115119b17ad49718652eff0e0f5b72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)b20bca8e1d/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c76cb745baa40cf96701b780790dacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)bca8e1d/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load .env file to get API Key\n",
    "load_dotenv()\n",
    "hf_token=os.getenv(\"hf_token\")\n",
    "\n",
    "#setup embeddings using HuggingFace\n",
    "embeddings  = HuggingFaceEmbeddings()\n",
    "\n",
    "#setup directory to store database on disk\n",
    "persist_dir = '../data/processed/chromadb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You must provide embeddings or a function to compute them",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m db \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39;49mfrom_documents(\n\u001b[1;32m      2\u001b[0m     documents, ebedding\u001b[39m=\u001b[39;49membeddings,\n\u001b[1;32m      3\u001b[0m     persist_directory\u001b[39m=\u001b[39;49mpersist_dir\n\u001b[1;32m      4\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/vectorstores/chroma.py:462\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m texts \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m    461\u001b[0m metadatas \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m--> 462\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_texts(\n\u001b[1;32m    463\u001b[0m     texts\u001b[39m=\u001b[39;49mtexts,\n\u001b[1;32m    464\u001b[0m     embedding\u001b[39m=\u001b[39;49membedding,\n\u001b[1;32m    465\u001b[0m     metadatas\u001b[39m=\u001b[39;49mmetadatas,\n\u001b[1;32m    466\u001b[0m     ids\u001b[39m=\u001b[39;49mids,\n\u001b[1;32m    467\u001b[0m     collection_name\u001b[39m=\u001b[39;49mcollection_name,\n\u001b[1;32m    468\u001b[0m     persist_directory\u001b[39m=\u001b[39;49mpersist_directory,\n\u001b[1;32m    469\u001b[0m     client_settings\u001b[39m=\u001b[39;49mclient_settings,\n\u001b[1;32m    470\u001b[0m     client\u001b[39m=\u001b[39;49mclient,\n\u001b[1;32m    471\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/vectorstores/chroma.py:430\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \n\u001b[1;32m    408\u001b[0m \u001b[39mIf a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[39m    Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    423\u001b[0m chroma_collection \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\n\u001b[1;32m    424\u001b[0m     collection_name\u001b[39m=\u001b[39mcollection_name,\n\u001b[1;32m    425\u001b[0m     embedding_function\u001b[39m=\u001b[39membedding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m     client\u001b[39m=\u001b[39mclient,\n\u001b[1;32m    429\u001b[0m )\n\u001b[0;32m--> 430\u001b[0m chroma_collection\u001b[39m.\u001b[39;49madd_texts(texts\u001b[39m=\u001b[39;49mtexts, metadatas\u001b[39m=\u001b[39;49mmetadatas, ids\u001b[39m=\u001b[39;49mids)\n\u001b[1;32m    431\u001b[0m \u001b[39mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/vectorstores/chroma.py:150\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function\u001b[39m.\u001b[39membed_documents(\u001b[39mlist\u001b[39m(texts))\n\u001b[0;32m--> 150\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_collection\u001b[39m.\u001b[39;49mupsert(\n\u001b[1;32m    151\u001b[0m     metadatas\u001b[39m=\u001b[39;49mmetadatas, embeddings\u001b[39m=\u001b[39;49membeddings, documents\u001b[39m=\u001b[39;49mtexts, ids\u001b[39m=\u001b[39;49mids\n\u001b[1;32m    152\u001b[0m )\n\u001b[1;32m    153\u001b[0m \u001b[39mreturn\u001b[39;00m ids\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/chromadb/api/models/Collection.py:295\u001b[0m, in \u001b[0;36mCollection.upsert\u001b[0;34m(self, ids, embeddings, metadatas, documents, increment_index)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupsert\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     increment_index: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    282\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Update the embeddings, metadatas or documents for provided ids, or create them if they don't exist.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \n\u001b[1;32m    285\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39m        None\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m     ids, embeddings, metadatas, documents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_embedding_set(\n\u001b[1;32m    296\u001b[0m         ids, embeddings, metadatas, documents\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39m_upsert(\n\u001b[1;32m    300\u001b[0m         collection_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid,\n\u001b[1;32m    301\u001b[0m         ids\u001b[39m=\u001b[39mids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         increment_index\u001b[39m=\u001b[39mincrement_index,\n\u001b[1;32m    306\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/chromadb/api/models/Collection.py:384\u001b[0m, in \u001b[0;36mCollection._validate_embedding_set\u001b[0;34m(self, ids, embeddings, metadatas, documents, require_embeddings_or_documents)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mif\u001b[39;00m embeddings \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m documents \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    385\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou must provide embeddings or a function to compute them\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m         )\n\u001b[1;32m    387\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function(documents)\n\u001b[1;32m    389\u001b[0m \u001b[39m# if embeddings is None:\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[39m#     raise ValueError(\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[39m#         \"Something went wrong. Embeddings should be computed at this point\"\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[39m#     )\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: You must provide embeddings or a function to compute them"
     ]
    }
   ],
   "source": [
    "db = Chroma.from_documents(\n",
    "    documents, ebedding=embeddings,\n",
    "    persist_directory=persist_dir\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently getting an error here, I think because I haven't passed it a true working API key yet. I'll keep doing some research on it because I don't want to run it without fully understanding it and rack up cost for no reason."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(path):\n",
    "    \"\"\"Return json file from specified filepath\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        json_file = json.load(f)\n",
    "    \n",
    "    return json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_chroma_documents(json_path):\n",
    "    \"\"\"Take json file and work through it to prepare for load to Chroma.\n",
    "    Instead of list comprehension, establis the blank list and loop through the json.\n",
    "    Then saves to the list using the langchain docstore.document -> Document modeul\n",
    "\n",
    "    This works specifically with the content and metadata we want for this project\"\"\"\n",
    "    docs = []\n",
    "    for item in get_json(json_path):\n",
    "        doc = Document(\n",
    "            page_content=f\"Table: {item['table']}\",\n",
    "            metadata={\n",
    "                'schema': item['schema'],\n",
    "                'table': item['table'],\n",
    "                'columns': json.dumps([col['c_name'] for col in item['columns']])\n",
    "            }\n",
    "        )\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Table: ACCOUNTS', metadata={'schema': 'small_bank_1', 'table': 'ACCOUNTS', 'columns': '[\"custid\", \"name\"]'}),\n",
       " Document(page_content='Table: AREA_CODE_STATE', metadata={'schema': 'voter_1', 'table': 'AREA_CODE_STATE', 'columns': '[\"area_code\", \"state\"]'}),\n",
       " Document(page_content='Table: Acceptance', metadata={'schema': 'workshop_paper', 'table': 'Acceptance', 'columns': '[\"Submission_ID\", \"Workshop_ID\", \"Result\"]'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = prep_chroma_documents('../data/interim/schema_info.json')\n",
    "\n",
    "documents[:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Schema Data to Vector Database\n",
    "\n",
    "My initial thought was to build a model that checks for similarity between the prompt and the schema information. But in doing research it sounds like this could be simplified and expedited using a vector database through langchain. We could then query the tables (with metadata) based on the question and return the documents that are most closely related. With this, we'll try to all be bundled into langchain. woohoo!\n",
    "\n",
    "Big shoutout to this great blogpost that provided some of the framework: https://canvasapp.com/blog/text-to-sql-in-production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(path):\n",
    "    \"\"\"Return json file from specified filepath\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        json_file = json.load(f)\n",
    "    \n",
    "    return json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_chroma_documents(json_path):\n",
    "    \"\"\"Take json file and work through it to prepare for load to Chroma.\n",
    "    Instead of list comprehension, establis the blank list and loop through the json.\n",
    "    Then saves to the list using the langchain docstore.document -> Document modeul\n",
    "\n",
    "    This works specifically with the content and metadata we want for this project\"\"\"\n",
    "    docs = []\n",
    "    for item in get_json(json_path):\n",
    "        doc = Document(\n",
    "            page_content=f\"Schema Table: {item['schema_split'] + ' ' + item['table_split']}\",\n",
    "            metadata={\n",
    "                'schema': item['schema'],\n",
    "                'table': item['table'],\n",
    "                'columns': json.dumps([col['c_name'] for col in item['columns']])\n",
    "            }\n",
    "        )\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Schema Table: academic author', metadata={'schema': 'academic', 'table': 'author', 'columns': '[\"aid\", \"homepage\", \"name\", \"oid\"]'}),\n",
       " Document(page_content='Schema Table: academic cite', metadata={'schema': 'academic', 'table': 'cite', 'columns': '[\"cited\", \"citing\"]'}),\n",
       " Document(page_content='Schema Table: academic conference', metadata={'schema': 'academic', 'table': 'conference', 'columns': '[\"cid\", \"homepage\", \"name\"]'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_info = prep_chroma_documents('../data/interim/schema_info.json')\n",
    "\n",
    "table_info[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Vector Database\n",
    "\n",
    "There are a few options for databases, but I'll go with Chroma because it is open source, makes local device use \"easy\", and has built-in connections with langchain.\n",
    "\n",
    "A key component in running apps using langchain is the ability store and work with embeddings, which is how AI models natively represent data of all kinds. Langchain will provide the application framework and Chroma will provide the vector store.\n",
    "\n",
    "Within that we can also do some information retrieval from the database, finding the most relevant tables based on the user question. This will allow us to engineer a better prompt to feed to our gpt chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup embeddings using HuggingFace\n",
    "embeddings  = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup directory to store database on disk\n",
    "persist_dir = '../data/processed/chromadb/schema-table-split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents=table_info, embedding=embeddings, persist_directory=persist_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist() #think I need to call this mainly because I'm in an ipynb.\n",
    "vectordb=None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Vector Database Query\n",
    "\n",
    "I'm going to start very simple and follow this first prompt through to the end. So for now, I'll only take the top result and roll with it. In the future we'll need to build out some contingency that can test multiple schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "vectordb = Chroma(persist_directory=persist_dir, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many heads of the departments are older than 56?\" #one of the prompts from the training data\n",
    "\n",
    "docs = vectordb.similarity_search(query, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Likely Schema and Table:\n",
      "\n",
      "department_management - head\n"
     ]
    }
   ],
   "source": [
    "print('Most Likely Schema and Table:\\n')\n",
    "i=0\n",
    "for doc in docs:\n",
    "    print(docs[i].metadata['schema'] + ' - ' + docs[i].metadata['table'])\n",
    "    i+=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and Next Steps\n",
    "\n",
    "This change appears to have worked on the first test example. In the first run with no processing and searching on a document that contained table title and metadata it gave me the wrong answer. But doing some preprocessing, splitting out any names that contained '_' into two words and restructuring the document to include 'schema table' looks to have helped a bit.\n",
    "\n",
    "I'll continue on with next steps, push this to main, and come back when I'm ready to complicate things."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

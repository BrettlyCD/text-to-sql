{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Schema Data to Vector Database\n",
    "\n",
    "My initial thought was to build a model that checks for similarity between the prompt and the schema information. But in doing research it sounds like this could be simplified and expedited using a vector database through langchain. We could then query the tables (with metadata) based on the question and return the documents that are most closely related. With this, we'll try to all be bundled into langchain. woohoo!\n",
    "\n",
    "Big shoutout to this great blogpost that provided some of the framework: https://canvasapp.com/blog/text-to-sql-in-production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(path):\n",
    "    \"\"\"Return json file from specified filepath\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        json_file = json.load(f)\n",
    "    \n",
    "    return json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_chroma_documents(json_path):\n",
    "    \"\"\"Take json file and work through it to prepare for load to Chroma.\n",
    "    Instead of list comprehension, establis the blank list and loop through the json.\n",
    "    Then saves to the list using the langchain docstore.document -> Document modeul\n",
    "\n",
    "    This works specifically with the content and metadata we want for this project\"\"\"\n",
    "    docs = []\n",
    "    for item in get_json(json_path):\n",
    "        doc = Document(\n",
    "            page_content=f\"Table: {item['table']}\",\n",
    "            metadata={\n",
    "                'schema': item['schema'],\n",
    "                'table': item['table'],\n",
    "                'columns': json.dumps([col['c_name'] for col in item['columns']])\n",
    "            }\n",
    "        )\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Table: ACCOUNTS', metadata={'schema': 'small_bank_1', 'table': 'ACCOUNTS', 'columns': '[\"custid\", \"name\"]'}),\n",
       " Document(page_content='Table: AREA_CODE_STATE', metadata={'schema': 'voter_1', 'table': 'AREA_CODE_STATE', 'columns': '[\"area_code\", \"state\"]'}),\n",
       " Document(page_content='Table: Acceptance', metadata={'schema': 'workshop_paper', 'table': 'Acceptance', 'columns': '[\"Submission_ID\", \"Workshop_ID\", \"Result\"]'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_info = prep_chroma_documents('../data/interim/schema_info.json')\n",
    "\n",
    "table_info[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Vector Database\n",
    "\n",
    "There are a few options for databases, but I'll go with Chroma because it is open source, makes local device use \"easy\", and has built-in connections with langchain.\n",
    "\n",
    "A key component in running apps using langchain is the ability store and work with embeddings, which is how AI models natively represent data of all kinds. Langchain will provide the application framework and Chroma will provide the vector store.\n",
    "\n",
    "Within that we can also do some information retrieval from the database, finding the most relevant tables based on the user question. This will allow us to engineer a better prompt to feed to our gpt chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup embeddings using HuggingFace\n",
    "embeddings  = HuggingFaceEmbeddings()\n",
    "\n",
    "#setup directory to store database on disk\n",
    "persist_dir = '../data/processed/chromadb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents=table_info, embedding=embeddings, persist_directory=persist_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist() #think I need to call this mainly because I'm in an ipynb.\n",
    "vectordb=None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Vector Database Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "vectordb = Chroma(persist_directory=persist_dir, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: Departments\n"
     ]
    }
   ],
   "source": [
    "query = \"How many heads of the departments are older than 56?\" #one of the prompts from the training data\n",
    "\n",
    "docs = vectordb.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Table: Departments', metadata={'schema': 'department_store', 'table': 'Departments', 'columns': '[\"department_id\", \"dept_store_id\", \"department_name\"]'}),\n",
       " Document(page_content='Table: Departments', metadata={'schema': 'student_transcripts_tracking', 'table': 'Departments', 'columns': '[\"department_id\", \"department_name\", \"department_description\", \"other_details\"]'}),\n",
       " Document(page_content='Table: departments', metadata={'schema': 'hr_1', 'table': 'departments', 'columns': '[\"DEPARTMENT_ID\", \"DEPARTMENT_NAME\", \"MANAGER_ID\", \"LOCATION_ID\"]'}),\n",
       " Document(page_content='Table: Department', metadata={'schema': 'hospital_1', 'table': 'Department', 'columns': '[\"DepartmentID\", \"Name\", \"Head\"]'})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 department_store\n",
      "2 student_transcripts_tracking\n",
      "3 hr_1\n",
      "4 hospital_1\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for doc in docs[:10]:\n",
    "    print(str(i+1) + ' ' + docs[i].metadata['schema'])\n",
    "    i+=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appears to be working in principle, although it isn't returning the correct answer. For that query we need to be pulling fomr the 'head' table underneath the department_management schema. I have a feeling this vector search is priritizing the document table which makes sense because the schema is only in the metadata. Maybe I could concatenate the schema and table together or load them in as equal parts of the documents so it looks at both variables equally when matchings? I'll research that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

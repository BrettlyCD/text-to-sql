{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Schema Data to Vector Database\n",
    "\n",
    "My initial thought was to build a model that checks for similarity between the prompt and the schema information. That may still be part of the long-term roadmap, but in doing research I found I could get acceptable results in a much simpler way using a vector database through langchain. By savings documents related to our schemas to this database, we can then query based on the question and return the documents that are most closely related. With this, we'll try to all be bundled into langchain. woohoo!\n",
    "\n",
    "Big shoutout to this great blogpost that provided some of the framework: https://canvasapp.com/blog/text-to-sql-in-production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2023-08-02T07:22:45.222238-07:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.4\n",
      "IPython version      : 8.14.0\n",
      "\n",
      "Compiler    : Clang 15.0.7 \n",
      "OS          : Darwin\n",
      "Release     : 22.5.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from watermark import watermark\n",
    "print(watermark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain import SQLDatabase\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from sqlalchemy import exc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Method\n",
    "\n",
    "In this first launch, I settled on saving the schema, table, column list, DDL, and 3 sample rows into each document.\n",
    "There is also metadata attached to each document with the schema, table, and column list to easily pull from the query results.\n",
    "\n",
    "## Establish Vector Database\n",
    "\n",
    "There are a few options for databases, but I'll go with Chroma because it is open source, makes local device use \"easy\", and has built-in connections with langchain.\n",
    "\n",
    "A key component in running apps using langchain is the ability store and work with embeddings, which is how AI models natively represent data of all kinds. Langchain will provide the application framework and Chroma will provide the vector store.\n",
    "\n",
    "Within that we can also do some information retrieval from the database, finding the most relevant tables based on the user question. This will allow us to engineer a better prompt to feed to our gpt chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup embeddings using HuggingFace\n",
    "embeddings  = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(path):\n",
    "    \"\"\"Return json file from specified filepath\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        json_file = json.load(f)\n",
    "    \n",
    "    return json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point to where our .sqlite files are saved\n",
    "db_path = '../data/processed/db/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_db(db_path, target_schema):\n",
    "    \"\"\"\n",
    "    Take in the identified schema and connect to the sqlite database with that name\n",
    "    \"\"\"\n",
    "    db_filepath = db_path\n",
    "    db_filename = target_schema + '.sqlite'\n",
    "\n",
    "    #point to database\n",
    "    base_dir = os.path.dirname(os.path.abspath(db_filepath+db_filename)) #get the full path within the device\n",
    "    db_path = os.path.join(base_dir, db_filename) #combine with filename to get db_path\n",
    "    db = SQLDatabase.from_uri(\"sqlite:///\" + db_path) #connect via the lanchain method\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define new document builder\n",
    "def prep_chroma_documents_v2(json_path):\n",
    "    \"\"\"Take json file and work through it to prepare for load to Chroma.\n",
    "    Instead of list comprehension, establis the blank list and loop through the json.\n",
    "    Then saves to the list using the langchain docstore.document -> Document module.\n",
    "\n",
    "    This version - adding the table info using the langchain SQLDatabase SQLAlchemy wrapper to get table info to add to metadata.\n",
    "    Would like to not reconnect to the database each time, but instead connect to each schema once and then loop through the tables. But I think this will be easier for now, even if it's less efficient.\n",
    "\n",
    "    This works specifically with the content and metadata we want for this project\"\"\"\n",
    "    docs = []\n",
    "    for item in get_json(json_path):\n",
    "        #connect to database\n",
    "        db = connect_db(db_path=db_path, target_schema=item['schema'])\n",
    "\n",
    "        #create variables\n",
    "        schema = item['schema']\n",
    "        table = item['table']\n",
    "        columns = json.dumps([col['c_name'] for col in item['columns']])\n",
    "        try:\n",
    "            table_info = db.get_table_info_no_throw(table_names=[table]) #put try-except here becasue there are some issues in the source sqlite database. I want to call this out, but continue.\n",
    "        except exc.SQLAlchemyError as e:\n",
    "            table_info = \"\"\n",
    "            print(schema + \"-\" + table + \": \" + str(e))\n",
    "        except TypeError as te:\n",
    "            print(schema + \"-\" + table + \": \" + str(te))          \n",
    "\n",
    "        #create document\n",
    "        doc = Document(\n",
    "            page_content=f\"\"\"\n",
    "Schema: {schema}\n",
    "Table: {table}\n",
    "Columns: {columns}\n",
    "DDL:\n",
    "    {table_info}\n",
    "\"\"\",\n",
    "            metadata={\n",
    "                'schema': schema,\n",
    "                'table': table,\n",
    "                'columns': columns,\n",
    "            }\n",
    "        )\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brettly/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/sql_database.py:111: SAWarning: Could not instantiate type <class 'sqlalchemy.sql.sqltypes.INTEGER'> with reflected arguments ['11']; using no arguments.\n",
      "  self._metadata.reflect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseball_1-all_star: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-appearances: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-batting: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-batting_postseason: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-college: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-fielding: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-fielding_outfield: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-fielding_postseason: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-hall_of_fame: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-home_game: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-manager: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-manager_award: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-manager_award_vote: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-manager_half: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-park: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-pitching: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-pitching_postseason: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-player: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-player_award: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-player_award_vote: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-player_college: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-postseason: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-salary: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-team: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-team_franchise: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "baseball_1-team_half: Could not initialize target column for ForeignKey 'player.team_id' on table 'fielding_postseason': table 'player' has no column named 'team_id'\n",
      "imdb-actor: Could not initialize target column for ForeignKey 'keyword.kid' on table 'tags': table 'keyword' has no column named 'kid'\n",
      "imdb-cast: Could not initialize target column for ForeignKey 'keyword.kid' on table 'tags': table 'keyword' has no column named 'kid'\n",
      "imdb-classification: Could not initialize target column for ForeignKey 'keyword.kid' on table 'tags': table 'keyword' has no column named 'kid'\n",
      "imdb-company: Could not initialize target column for ForeignKey 'keyword.kid' on table 'tags': table 'keyword' has no column named 'kid'\n",
      "imdb-copyright: Could not initialize target column for ForeignKey 'keyword.kid' on table 'tags': table 'keyword' has no column named 'kid'\n",
      "imdb-directed_by: Could not initialize target column for ForeignKey 'keyword.kid' on table 'tags': table 'keyword' has no column named 'kid'\n",
      "imdb-director: Could not initialize target column for ForeignKey 'keyword.kid' on table 'tags': table 'keyword' has no column named 'kid'\n",
      "imdb-genre: Could not initialize target column for ForeignKey 'keyword.kid' on table 'tags': table 'keyword' has no column named 'kid'\n",
      "imdb-keyword: Could not initialize target column for ForeignKey 'keyword.kid' on table 'tags': table 'keyword' has no column named 'kid'\n",
      "imdb-made_by: Could not initialize target column for ForeignKey 'keyword.kid' on table 'tags': table 'keyword' has no column named 'kid'\n",
      "imdb-movie: Could not initialize target column for ForeignKey 'keyword.kid' on table 'tags': table 'keyword' has no column named 'kid'\n",
      "imdb-producer: Could not initialize target column for ForeignKey 'keyword.kid' on table 'tags': table 'keyword' has no column named 'kid'\n",
      "imdb-tags: Could not initialize target column for ForeignKey 'keyword.kid' on table 'tags': table 'keyword' has no column named 'kid'\n",
      "imdb-tv_series: Could not initialize target column for ForeignKey 'keyword.kid' on table 'tags': table 'keyword' has no column named 'kid'\n",
      "imdb-writer: Could not initialize target column for ForeignKey 'keyword.kid' on table 'tags': table 'keyword' has no column named 'kid'\n",
      "imdb-written_by: Could not initialize target column for ForeignKey 'keyword.kid' on table 'tags': table 'keyword' has no column named 'kid'\n",
      "loan_1-bank: Could not initialize target column for ForeignKey 'customer.Cust_ID' on table 'loan': table 'customer' has no column named 'Cust_ID'\n",
      "loan_1-customer: Could not initialize target column for ForeignKey 'customer.Cust_ID' on table 'loan': table 'customer' has no column named 'Cust_ID'\n",
      "loan_1-loan: Could not initialize target column for ForeignKey 'customer.Cust_ID' on table 'loan': table 'customer' has no column named 'Cust_ID'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brettly/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/sql_database.py:111: SAWarning: WARNING: SQL-parsed foreign key constraint '('Cust_ID', 'customer', 'Cust_ID')' could not be located in PRAGMA foreign_keys for table loan\n",
      "  self._metadata.reflect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurants-GEOGRAPHIC: Could not initialize target column for ForeignKey 'RESTAURANT.RESTAURANT_ID' on table 'LOCATION': table 'RESTAURANT' has no column named 'RESTAURANT_ID'\n",
      "restaurants-LOCATION: Could not initialize target column for ForeignKey 'RESTAURANT.RESTAURANT_ID' on table 'LOCATION': table 'RESTAURANT' has no column named 'RESTAURANT_ID'\n",
      "restaurants-RESTAURANT: Could not initialize target column for ForeignKey 'RESTAURANT.RESTAURANT_ID' on table 'LOCATION': table 'RESTAURANT' has no column named 'RESTAURANT_ID'\n",
      "sakila_1-film: (in table 'film', column 'rating'): Can't generate DDL for NullType(); did you forget to specify a type on this Column?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brettly/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/sql_database.py:111: SAWarning: WARNING: SQL-parsed foreign key constraint '('store_id', 'store', 'store_id')' could not be located in PRAGMA foreign_keys for table staff\n",
      "  self._metadata.reflect(\n",
      "/Users/brettly/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/sql_database.py:111: SAWarning: WARNING: SQL-parsed foreign key constraint '('Event_ID', 'Events', 'Event_ID')' could not be located in PRAGMA foreign_keys for table Assets_in_Events\n",
      "  self._metadata.reflect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_product-district: Could not initialize target column for ForeignKey 'product.Product_ID' on table 'store_product': table 'product' has no column named 'Product_ID'\n",
      "store_product-product: Could not initialize target column for ForeignKey 'product.Product_ID' on table 'store_product': table 'product' has no column named 'Product_ID'\n",
      "store_product-store: Could not initialize target column for ForeignKey 'product.Product_ID' on table 'store_product': table 'product' has no column named 'Product_ID'\n",
      "store_product-store_district: Could not initialize target column for ForeignKey 'product.Product_ID' on table 'store_product': table 'product' has no column named 'Product_ID'\n",
      "store_product-store_product: Could not initialize target column for ForeignKey 'product.Product_ID' on table 'store_product': table 'product' has no column named 'Product_ID'\n",
      "wta_1-matches: fromisoformat: argument must be str\n",
      "wta_1-players: fromisoformat: argument must be str\n",
      "wta_1-rankings: fromisoformat: argument must be str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brettly/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/sql_database.py:111: SAWarning: Could not instantiate type <class 'sqlalchemy.sql.sqltypes.BIGINT'> with reflected arguments ['20']; using no arguments.\n",
      "  self._metadata.reflect(\n"
     ]
    }
   ],
   "source": [
    "table_docs = prep_chroma_documents_v2('../data/interim/schema_info.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup directory to store database on disk\n",
    "persist_dir_new = '../data/processed/chromadb/schema-table-info'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "###commenting out since this was already run and created###\n",
    "\n",
    "#vectordb_new = Chroma.from_documents(documents=table_docs, embedding=embeddings, persist_directory=persist_dir_new)\n",
    "\n",
    "#vectordb_new.persist() #should only need to run this out of the notebook, not a .py file.\n",
    "vectordb_new=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test - Table Info Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\nSchema: hr_1\\nTable: departments\\nColumns: [\"DEPARTMENT_ID\", \"DEPARTMENT_NAME\", \"MANAGER_ID\", \"LOCATION_ID\"]\\nDDL:\\n    \\nCREATE TABLE departments (\\n\\t\"DEPARTMENT_ID\" DECIMAL(4, 0) DEFAULT \\'0\\' NOT NULL, \\n\\t\"DEPARTMENT_NAME\" VARCHAR(30) NOT NULL, \\n\\t\"MANAGER_ID\" DECIMAL(6, 0) DEFAULT NULL, \\n\\t\"LOCATION_ID\" DECIMAL(4, 0) DEFAULT NULL, \\n\\tPRIMARY KEY (\"DEPARTMENT_ID\")\\n)\\n\\n/*\\n3 rows from departments table:\\nDEPARTMENT_ID\\tDEPARTMENT_NAME\\tMANAGER_ID\\tLOCATION_ID\\n10\\tAdministration\\t200\\t1700\\n20\\tMarketing\\t201\\t1800\\n30\\tPurchasing\\t114\\t1700\\n*/\\n', metadata={'schema': 'hr_1', 'table': 'departments', 'columns': '[\"DEPARTMENT_ID\", \"DEPARTMENT_NAME\", \"MANAGER_ID\", \"LOCATION_ID\"]'}),\n",
       " Document(page_content='\\nSchema: department_management\\nTable: department\\nColumns: [\"Department_ID\", \"Name\", \"Creation\", \"Ranking\", \"Budget_in_Billions\", \"Num_Employees\"]\\nDDL:\\n    \\nCREATE TABLE department (\\n\\t\"Department_ID\" INTEGER, \\n\\t\"Name\" TEXT, \\n\\t\"Creation\" TEXT, \\n\\t\"Ranking\" INTEGER, \\n\\t\"Budget_in_Billions\" REAL, \\n\\t\"Num_Employees\" REAL, \\n\\tPRIMARY KEY (\"Department_ID\")\\n)\\n\\n/*\\n3 rows from department table:\\nDepartment_ID\\tName\\tCreation\\tRanking\\tBudget_in_Billions\\tNum_Employees\\n1\\tState\\t1789\\t1\\t9.96\\t30266.0\\n2\\tTreasury\\t1789\\t2\\t11.1\\t115897.0\\n3\\tDefense\\t1947\\t3\\t439.3\\t3000000.0\\n*/\\n', metadata={'schema': 'department_management', 'table': 'department', 'columns': '[\"Department_ID\", \"Name\", \"Creation\", \"Ranking\", \"Budget_in_Billions\", \"Num_Employees\"]'}),\n",
       " Document(page_content='\\nSchema: hr_1\\nTable: jobs\\nColumns: [\"JOB_ID\", \"JOB_TITLE\", \"MIN_SALARY\", \"MAX_SALARY\"]\\nDDL:\\n    \\nCREATE TABLE jobs (\\n\\t\"JOB_ID\" VARCHAR(10) DEFAULT \\'\\' NOT NULL, \\n\\t\"JOB_TITLE\" VARCHAR(35) NOT NULL, \\n\\t\"MIN_SALARY\" DECIMAL(6, 0) DEFAULT NULL, \\n\\t\"MAX_SALARY\" DECIMAL(6, 0) DEFAULT NULL, \\n\\tPRIMARY KEY (\"JOB_ID\")\\n)\\n\\n/*\\n3 rows from jobs table:\\nJOB_ID\\tJOB_TITLE\\tMIN_SALARY\\tMAX_SALARY\\nAD_PRES\\tPresident\\t20000\\t40000\\nAD_VP\\tAdministration Vice President\\t15000\\t30000\\nAD_ASST\\tAdministration Assistant\\t3000\\t6000\\n*/\\n', metadata={'schema': 'hr_1', 'table': 'jobs', 'columns': '[\"JOB_ID\", \"JOB_TITLE\", \"MIN_SALARY\", \"MAX_SALARY\"]'})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load from disk\n",
    "vectordb_info = Chroma(persist_directory=persist_dir_new, embedding_function=embeddings)\n",
    "\n",
    "test = \"What is the average number of employees of the departments whose rank is between 10 and 15?\" #one of the prompts from the training data\n",
    "\n",
    "test_docs = vectordb_info.similarity_search(test, k=3)\n",
    "\n",
    "test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\nSchema: hr_1\\nTable: departments\\nColumns: [\"DEPARTMENT_ID\", \"DEPARTMENT_NAME\", \"MANAGER_ID\", \"LOCATION_ID\"]\\nDDL:\\n    \\nCREATE TABLE departments (\\n\\t\"DEPARTMENT_ID\" DECIMAL(4, 0) DEFAULT \\'0\\' NOT NULL, \\n\\t\"DEPARTMENT_NAME\" VARCHAR(30) NOT NULL, \\n\\t\"MANAGER_ID\" DECIMAL(6, 0) DEFAULT NULL, \\n\\t\"LOCATION_ID\" DECIMAL(4, 0) DEFAULT NULL, \\n\\tPRIMARY KEY (\"DEPARTMENT_ID\")\\n)\\n\\n/*\\n3 rows from departments table:\\nDEPARTMENT_ID\\tDEPARTMENT_NAME\\tMANAGER_ID\\tLOCATION_ID\\n10\\tAdministration\\t200\\t1700\\n20\\tMarketing\\t201\\t1800\\n30\\tPurchasing\\t114\\t1700\\n*/\\n', metadata={'schema': 'hr_1', 'table': 'departments', 'columns': '[\"DEPARTMENT_ID\", \"DEPARTMENT_NAME\", \"MANAGER_ID\", \"LOCATION_ID\"]'}),\n",
       " Document(page_content='\\nSchema: insurance_and_eClaims\\nTable: Staff\\nColumns: [\"Staff_ID\", \"Staff_Details\"]\\nDDL:\\n    \\nCREATE TABLE \"Staff\" (\\n\\t\"Staff_ID\" INTEGER NOT NULL, \\n\\t\"Staff_Details\" VARCHAR(255) NOT NULL, \\n\\tPRIMARY KEY (\"Staff_ID\")\\n)\\n\\n/*\\n3 rows from Staff table:\\nStaff_ID\\tStaff_Details\\n406\\tClifton\\n427\\tCathryn\\n510\\tKaci\\n*/\\n', metadata={'schema': 'insurance_and_eClaims', 'table': 'Staff', 'columns': '[\"Staff_ID\", \"Staff_Details\"]'}),\n",
       " Document(page_content='\\nSchema: company_1\\nTable: employee\\nColumns: [\"Fname\", \"Minit\", \"Lname\", \"Ssn\", \"Bdate\", \"Address\", \"Sex\", \"Salary\", \"Super_ssn\", \"Dno\"]\\nDDL:\\n    \\nCREATE TABLE employee (\\n\\t\"Fname\" TEXT, \\n\\t\"Minit\" TEXT, \\n\\t\"Lname\" TEXT, \\n\\t\"Ssn\" INTEGER, \\n\\t\"Bdate\" TEXT, \\n\\t\"Address\" TEXT, \\n\\t\"Sex\" TEXT, \\n\\t\"Salary\" INTEGER, \\n\\t\"Super_ssn\" INTEGER, \\n\\t\"Dno\" INTEGER, \\n\\tPRIMARY KEY (\"Ssn\")\\n)\\n\\n/*\\n3 rows from employee table:\\nFname\\tMinit\\tLname\\tSsn\\tBdate\\tAddress\\tSex\\tSalary\\tSuper_ssn\\tDno\\nJonh\\tB\\tSmith\\t123456789\\t1965-01-09\\t731 Fondren, Houston, TX\\tM\\t30000\\t333445555\\t5\\nFranklin\\tT\\tWong\\t333445555\\t1955-12-08\\t638 Voss, Houston, TX\\tM\\t40000\\t888665555\\t5\\nJoyce\\tA\\tEnglish\\t453453453\\t1972-07-31\\t5631 Rice, Houston, TX\\tF\\t25000\\t333445555\\t5\\n*/\\n', metadata={'schema': 'company_1', 'table': 'employee', 'columns': '[\"Fname\", \"Minit\", \"Lname\", \"Ssn\", \"Bdate\", \"Address\", \"Sex\", \"Salary\", \"Super_ssn\", \"Dno\"]'}),\n",
       " Document(page_content='\\nSchema: college_3\\nTable: Faculty\\nColumns: [\"FacID\", \"Lname\", \"Fname\", \"Rank\", \"Sex\", \"Phone\", \"Room\", \"Building\"]\\nDDL:\\n    \\nCREATE TABLE \"Faculty\" (\\n\\t\"FacID\" INTEGER, \\n\\t\"Lname\" VARCHAR(15), \\n\\t\"Fname\" VARCHAR(15), \\n\\t\"Rank\" VARCHAR(15), \\n\\t\"Sex\" VARCHAR(1), \\n\\t\"Phone\" INTEGER, \\n\\t\"Room\" VARCHAR(5), \\n\\t\"Building\" VARCHAR(13), \\n\\tPRIMARY KEY (\"FacID\")\\n)\\n\\n/*\\n3 rows from Faculty table:\\nFacID\\tLname\\tFname\\tRank\\tSex\\tPhone\\tRoom\\tBuilding\\n1082\\tGiuliano\\tMark\\tInstructor\\tM\\t2424\\t224\\tNEB\\n1121\\tGoodrich\\tMichael\\tProfessor\\tM\\t3593\\t219\\tNEB\\n1148\\tMasson\\tGerald\\tProfessor\\tM\\t3402\\t224B\\tNEB\\n*/\\n', metadata={'schema': 'college_3', 'table': 'Faculty', 'columns': '[\"FacID\", \"Lname\", \"Fname\", \"Rank\", \"Sex\", \"Phone\", \"Room\", \"Building\"]'})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test mmr retriever mentioned in langchain docs (Maximum Marginal Relevance)\n",
    "retriever = vectordb_info.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "retriever.get_relevant_documents(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This didn't put it in the 1st spot, but it is in the top three. After testing on all of the questions, this document structure ended up being the best performing of the three methods I tried. This is despite the fact that some of the table info steps didn't run due to database issues for 3-4 of the tables.\n",
    "\n",
    "## Other Methods\n",
    "\n",
    "### Simple Schema-Table\n",
    "\n",
    "In this first iteration I tried, the document content only had the schema-table combo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_chroma_documents(json_path):\n",
    "    \"\"\"Take json file and work through it to prepare for load to Chroma.\n",
    "    Instead of list comprehension, establis the blank list and loop through the json.\n",
    "    Then saves to the list using the langchain docstore.document -> Document modeul\n",
    "\n",
    "    This works specifically with the content and metadata we want for this project\"\"\"\n",
    "    docs = []\n",
    "    for item in get_json(json_path):\n",
    "        doc = Document(\n",
    "            page_content=f\"Schema Table: {item['schema_split'] + ' ' + item['table_split']}\",\n",
    "            metadata={\n",
    "                'schema': item['schema'],\n",
    "                'table': item['table'],\n",
    "                'columns': json.dumps([col['c_name'] for col in item['columns']])\n",
    "            }\n",
    "        )\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Schema Table: academic author', metadata={'schema': 'academic', 'table': 'author', 'columns': '[\"aid\", \"homepage\", \"name\", \"oid\"]'}),\n",
       " Document(page_content='Schema Table: academic cite', metadata={'schema': 'academic', 'table': 'cite', 'columns': '[\"cited\", \"citing\"]'}),\n",
       " Document(page_content='Schema Table: academic conference', metadata={'schema': 'academic', 'table': 'conference', 'columns': '[\"cid\", \"homepage\", \"name\"]'})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_info = prep_chroma_documents('../data/interim/schema_info.json')\n",
    "\n",
    "table_info[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup directory to store database on disk\n",
    "persist_dir = '../data/processed/chromadb/schema-table-split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###commenting out since we already created this database\n",
    "\n",
    "#vectordb = Chroma.from_documents(documents=table_info, embedding=embeddings, persist_directory=persist_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectordb.persist() #think I need to call this mainly because I'm in an ipynb.\n",
    "vectordb=None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Vector Database Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "vectordb = Chroma(persist_directory=persist_dir, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many heads of the departments are older than 56?\" #one of the prompts from the training data\n",
    "\n",
    "docs = vectordb.similarity_search(query, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Likely Schema and Table:\n",
      "\n",
      "department_management - head\n"
     ]
    }
   ],
   "source": [
    "print('Most Likely Schema and Table:\\n')\n",
    "i=0\n",
    "for doc in docs:\n",
    "    print(docs[i].metadata['schema'] + ' - ' + docs[i].metadata['table'])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Entity Extraction\n",
    "\n",
    "https://canvasapp.com/blog/text-to-sql-in-production\n",
    "\n",
    "This blog suggests doing some entity extraction from the question. I can try the same method they do of calling to a chat LLM. I could also try using a HuggingFace model specific to text classification or entity extraction.\n",
    "\n",
    "I could try to train an entity recognition model on the schema information. But first I'll try using a chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish examples of what we want to get back from the model\n",
    "\n",
    "extract_prompt = \"\"\"question: How many teams are there?\n",
    "response: team\n",
    "\n",
    "question: What user spent the most money in March?\n",
    "response: user, money\n",
    "\n",
    "question: What is the name of the instructor who advises the student with the greatest number of total credits?\n",
    "response: instructor, advisor, student, credits\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instruction for the LLM\n",
    "\n",
    "instruction = \"\"\"\n",
    "For a given question, determine the keywords for determining what tables should be used to write a SQL query to answer the question.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the full prompt content combining the examples and the instruction\n",
    "\n",
    "prompt = (\n",
    "    instruction\n",
    "    + \"\\nHere are are three question/response examples: \"\n",
    "    + extract_prompt\n",
    "    + \"\\n\\nquestion: \"\n",
    "    + test\n",
    "    + \"\\nresponse:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#structure the prompt in the langchain template\n",
    "\n",
    "entity_extr_prompt = PromptTemplate(\n",
    "    input_variables=[],\n",
    "    template = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish the LLM - using google flan again\n",
    "\n",
    "#get api key\n",
    "load_dotenv()\n",
    "hf_api_token = os.getenv('hf_token')\n",
    "\n",
    "#add path to HF repo\n",
    "repo_id = 'tiiuae/falcon-7b-instruct'\n",
    "\n",
    "#establish llm model\n",
    "llm = HuggingFaceHub(repo_id=repo_id, huggingfacehub_api_token=hf_api_token, model_kwargs={\"temperature\": .005, \"max_length\": 512})\n",
    "\n",
    "#create an LLM chain\n",
    "chain = LLMChain(llm=llm, prompt=entity_extr_prompt, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " average, rank, employee, departments\n"
     ]
    }
   ],
   "source": [
    "#test on question\n",
    "results= chain.predict() #run llm\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\nSchema: department_management\\nTable: department\\nColumns: [\"Department_ID\", \"Name\", \"Creation\", \"Ranking\", \"Budget_in_Billions\", \"Num_Employees\"]\\nDDL:\\n    \\nCREATE TABLE department (\\n\\t\"Department_ID\" INTEGER, \\n\\t\"Name\" TEXT, \\n\\t\"Creation\" TEXT, \\n\\t\"Ranking\" INTEGER, \\n\\t\"Budget_in_Billions\" REAL, \\n\\t\"Num_Employees\" REAL, \\n\\tPRIMARY KEY (\"Department_ID\")\\n)\\n\\n/*\\n3 rows from department table:\\nDepartment_ID\\tName\\tCreation\\tRanking\\tBudget_in_Billions\\tNum_Employees\\n1\\tState\\t1789\\t1\\t9.96\\t30266.0\\n2\\tTreasury\\t1789\\t2\\t11.1\\t115897.0\\n3\\tDefense\\t1947\\t3\\t439.3\\t3000000.0\\n*/\\n', metadata={'schema': 'department_management', 'table': 'department', 'columns': '[\"Department_ID\", \"Name\", \"Creation\", \"Ranking\", \"Budget_in_Billions\", \"Num_Employees\"]'}),\n",
       " Document(page_content='\\nSchema: hr_1\\nTable: departments\\nColumns: [\"DEPARTMENT_ID\", \"DEPARTMENT_NAME\", \"MANAGER_ID\", \"LOCATION_ID\"]\\nDDL:\\n    \\nCREATE TABLE departments (\\n\\t\"DEPARTMENT_ID\" DECIMAL(4, 0) DEFAULT \\'0\\' NOT NULL, \\n\\t\"DEPARTMENT_NAME\" VARCHAR(30) NOT NULL, \\n\\t\"MANAGER_ID\" DECIMAL(6, 0) DEFAULT NULL, \\n\\t\"LOCATION_ID\" DECIMAL(4, 0) DEFAULT NULL, \\n\\tPRIMARY KEY (\"DEPARTMENT_ID\")\\n)\\n\\n/*\\n3 rows from departments table:\\nDEPARTMENT_ID\\tDEPARTMENT_NAME\\tMANAGER_ID\\tLOCATION_ID\\n10\\tAdministration\\t200\\t1700\\n20\\tMarketing\\t201\\t1800\\n30\\tPurchasing\\t114\\t1700\\n*/\\n', metadata={'schema': 'hr_1', 'table': 'departments', 'columns': '[\"DEPARTMENT_ID\", \"DEPARTMENT_NAME\", \"MANAGER_ID\", \"LOCATION_ID\"]'}),\n",
       " Document(page_content='\\nSchema: department_store\\nTable: Staff_Department_Assignments\\nColumns: [\"staff_id\", \"department_id\", \"date_assigned_from\", \"job_title_code\", \"date_assigned_to\"]\\nDDL:\\n    \\nCREATE TABLE \"Staff_Department_Assignments\" (\\n\\tstaff_id INTEGER NOT NULL, \\n\\tdepartment_id INTEGER NOT NULL, \\n\\tdate_assigned_from DATETIME NOT NULL, \\n\\tjob_title_code VARCHAR(10) NOT NULL, \\n\\tdate_assigned_to DATETIME, \\n\\tPRIMARY KEY (staff_id, department_id), \\n\\tFOREIGN KEY(staff_id) REFERENCES \"Staff\" (staff_id), \\n\\tFOREIGN KEY(department_id) REFERENCES \"Departments\" (department_id)\\n)\\n\\n/*\\n3 rows from Staff_Department_Assignments table:\\nstaff_id\\tdepartment_id\\tdate_assigned_from\\tjob_title_code\\tdate_assigned_to\\n5\\t4\\t2017-06-11 22:55:20\\tDepartment Manager\\t2018-03-23 21:59:11\\n10\\t5\\t2017-12-18 19:12:15\\tSales Person\\t2018-03-23 20:25:24\\n1\\t5\\t2018-02-14 03:15:29\\tClerical Staff\\t2018-03-24 19:57:56\\n*/\\n', metadata={'schema': 'department_store', 'table': 'Staff_Department_Assignments', 'columns': '[\"staff_id\", \"department_id\", \"date_assigned_from\", \"job_title_code\", \"date_assigned_to\"]'})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run the resul throug the similarity search\n",
    "vectordb_info.similarity_search(results, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This single test is promising as it's the first to return our target schema as the primary result. I'm worried about how to efficiently test this, since each run requires a call to an LLM. Maybe I can go to my validation page and just test on a sample of the questions.\n",
    "\n",
    "### Follow-Up:\n",
    "\n",
    "I was able to test on 50 of the random questions - only getting 40% of them correctly matched. Trying to go for a larger sample size ran into the hourly limits of Hugging Face Inference. Due to the poor performance on the small sample size and inability to test more, I won't go with this method.\n",
    "\n",
    "## Final Test -> Remove Table Info from Docs\n",
    "\n",
    "This will just make the docs have the schema - table - columns to see if that improves the overall results. If not, I'll go with my v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define new document builder\n",
    "def prep_chroma_documents_v3(json_path):\n",
    "    \"\"\"Take json file and work through it to prepare for load to Chroma.\n",
    "    Instead of list comprehension, establis the blank list and loop through the json.\n",
    "    Then saves to the list using the langchain docstore.document -> Document module.\n",
    "\n",
    "    This version - adding the table info using the langchain SQLDatabase SQLAlchemy wrapper to get table info to add to metadata.\n",
    "    Would like to not reconnect to the database each time, but instead connect to each schema once and then loop through the tables. But I think this will be easier for now, even if it's less efficient.\n",
    "\n",
    "    This works specifically with the content and metadata we want for this project\"\"\"\n",
    "    docs = []\n",
    "    for item in get_json(json_path):\n",
    "        #create variables\n",
    "        schema = item['schema']\n",
    "        table = item['table']\n",
    "        columns = json.dumps([col['c_name'] for col in item['columns']])      \n",
    "\n",
    "        #create document\n",
    "        doc = Document(\n",
    "            page_content=f\"\"\"\n",
    "Schema: {schema}\n",
    "Table: {table}\n",
    "Columns: {columns}\n",
    "\"\"\",\n",
    "            metadata={\n",
    "                'schema': schema,\n",
    "                'table': table,\n",
    "                'columns': columns,\n",
    "            }\n",
    "        )\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new docs\n",
    "new_docs = prep_chroma_documents_v3('../data/interim/schema_info.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\nSchema: academic\\nTable: author\\nColumns: [\"aid\", \"homepage\", \"name\", \"oid\"]\\n', metadata={'schema': 'academic', 'table': 'author', 'columns': '[\"aid\", \"homepage\", \"name\", \"oid\"]'}),\n",
       " Document(page_content='\\nSchema: academic\\nTable: cite\\nColumns: [\"cited\", \"citing\"]\\n', metadata={'schema': 'academic', 'table': 'cite', 'columns': '[\"cited\", \"citing\"]'}),\n",
       " Document(page_content='\\nSchema: academic\\nTable: conference\\nColumns: [\"cid\", \"homepage\", \"name\"]\\n', metadata={'schema': 'academic', 'table': 'conference', 'columns': '[\"cid\", \"homepage\", \"name\"]'})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_docs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup directory to store database on disk\n",
    "persist_dir_3 = '../data/processed/chromadb/sch-tab-col'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "###commenting out as this was already created\n",
    "\n",
    "#vectordb_3 = Chroma.from_documents(documents=new_docs, embedding=embeddings, persist_directory=persist_dir_3)\n",
    "\n",
    "#vectordb_3.persist() #think I need to call this mainly because I'm in an ipynb.\n",
    "vectordb_3=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test New DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\nSchema: department_management\\nTable: department\\nColumns: [\"Department_ID\", \"Name\", \"Creation\", \"Ranking\", \"Budget_in_Billions\", \"Num_Employees\"]\\n', metadata={'schema': 'department_management', 'table': 'department', 'columns': '[\"Department_ID\", \"Name\", \"Creation\", \"Ranking\", \"Budget_in_Billions\", \"Num_Employees\"]'}),\n",
       " Document(page_content='\\nSchema: department_store\\nTable: Staff_Department_Assignments\\nColumns: [\"staff_id\", \"department_id\", \"date_assigned_from\", \"job_title_code\", \"date_assigned_to\"]\\n', metadata={'schema': 'department_store', 'table': 'Staff_Department_Assignments', 'columns': '[\"staff_id\", \"department_id\", \"date_assigned_from\", \"job_title_code\", \"date_assigned_to\"]'}),\n",
       " Document(page_content='\\nSchema: department_management\\nTable: management\\nColumns: [\"department_ID\", \"head_ID\", \"temporary_acting\"]\\n', metadata={'schema': 'department_management', 'table': 'management', 'columns': '[\"department_ID\", \"head_ID\", \"temporary_acting\"]'})]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load from disk\n",
    "vectordb_3 = Chroma(persist_directory=persist_dir_3, embedding_function=embeddings)\n",
    "\n",
    "test = \"What is the average number of employees of the departments whose rank is between 10 and 15?\" #one of the prompts from the training data\n",
    "\n",
    "test_docs = vectordb_3.similarity_search(test, k=3)\n",
    "\n",
    "test_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "Looks good on the test, I'll go to my validation notebook and try it on all of the questions.\n",
    "\n",
    "### Follow-up:\n",
    "\n",
    "I did the test on the full question list and it returned slightly worse results than the db with the table info. I'll stick with that one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates\n",
    "\n",
    "Now that I have my vector database setup, I want to pull down the most likely schema and table and save the metadata as variables. Then I think the best way to proceed to is to create a prompt template that accepts the variables and allows a consistent entry point to the model.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Top Results from VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup embeddings using HuggingFace and the directory location\n",
    "embeddings  = HuggingFaceEmbeddings()\n",
    "persist_dir = '../data/processed/chromadb/schema-table-split'\n",
    "\n",
    "# load from disk\n",
    "vectordb = Chroma(persist_directory=persist_dir, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run prompt as query and get most likely results\n",
    "query = \"How many heads of the departments are older than 56?\" #first prompt from the training data\n",
    "\n",
    "result = vectordb.similarity_search(query, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Schema Table: department management head', metadata={'schema': 'department_management', 'table': 'head', 'columns': '[\"head_id\", \"name\", \"born_state\", \"age\"]'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Metadata to Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department_management head [\"head_id\", \"name\", \"born_state\", \"age\"]\n"
     ]
    }
   ],
   "source": [
    "top_schema = result[0].metadata['schema']\n",
    "top_table = result[0].metadata['table']\n",
    "table_cols = result[0].metadata['columns']\n",
    "\n",
    "print(top_schema, top_table, table_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SQL Database Chain\n",
    "\n",
    "Langchain has a great SQL Agent we can tap into. It allows for connecting to huggingface models, although it warns about not being able to run complicated tasks due to the lack of processing power. For the simplest test, what we'll need to do is point to the database (in this case the schema returned by our vector query), establish which LLM model we're using, and run the prompt on the chain. We may even use the option to specify the table to give it very little thinking to do.\n",
    "\n",
    "There is also more advanced features we'll tap into down the road, like Sequential Chaining that takes the database and then links tables together to find the right answer and Query Checker that can solve for some issues in the query. But we'll get to those later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Chain Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SQLDatabase, SQLDatabaseChain\n",
    "import logging\n",
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "device_id = -1  # default to no-GPU, but use GPU and half precision mode if available\n",
    "if torch.cuda.is_available():\n",
    "    device_id = 0\n",
    "    try:\n",
    "        model = model_id.half()\n",
    "    except RuntimeError as exc:\n",
    "        logging.warn(f\"Could not run model in half precision mode: {str(exc)}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1024, device=device_id, trust_remote_code=True)\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appears to be working when I start it up, but I haven't ran it all the way through since I don't fully understand all the possibilities when it comes to using a huggingface model. Is there a way to access private non-OpenAI models?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

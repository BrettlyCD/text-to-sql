{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy .sqlite files to common directory\n",
    "\n",
    "Use terminal commands to copy from data/raw/spider/database/ to data/processed/db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###commented out so we don't run again###\n",
    "#create a 'db' directory in the data/processed folder\n",
    "#! mkdir ../data/processed/db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###commented out so we don't run again###\n",
    "#run command to move all .sqlite folders to our newly created directory\n",
    "\n",
    "# src_dir = \"/Users/brettly/Sboard/projects/text-to-sql/data/raw/spider/database\"\n",
    "# dst_dir = \"/Users/brettly/Sboard/projects/text-to-sql/data/processed/db\"\n",
    "# for root, dirs, files in os.walk(src_dir):\n",
    "#     for f in files:\n",
    "#         if f.endswith('.sqlite'):\n",
    "#             shutil.copy(os.path.join(root,f), dst_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Functions For Getting Schema Info\n",
    "\n",
    "I already went through this with a really complicated script to get the details from a supplied json file. But that was with the itention of getting everything into a PostgreSQL database. Right now I want to prioritize the application and model itself rather than fussing with the data, so I'm going to pivot to using the supplies .sqlite files.\n",
    "\n",
    "So while I already have some schema info, it was really lacking in the supplied dtypes (only text and number), so I'm going to cleanup the process and build functions to extract the info directly from the databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(filepath, filetype):\n",
    "    \"\"\"Create empty list, loop through files within a directory and grab those of a specifed filetype. Append those to the empty list and return without the filetypes.\n",
    "    filepath example: \"../data/processed/db/\"\n",
    "    filetype example: \".sqlite\"\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        for file in files:\n",
    "            if file.endswith(filetype):\n",
    "                file_list.append(os.path.join(file))\n",
    "\n",
    "    filenames = [file.replace(filetype, \"\") for file in file_list]\n",
    "\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_names(db_path):\n",
    "    \"\"\"Function to use within build_schema_info to pull down table names from given schema, loop through them, and save to a list\"\"\"\n",
    "    \n",
    "    table_list = []\n",
    "\n",
    "    db = sqlite3.connect(db_path)\n",
    "    cursor=db.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "\n",
    "    for table in tables:\n",
    "        table_list.append(table[0])\n",
    "    \n",
    "    cursor.close()\n",
    "    db.close()\n",
    "    \n",
    "    return table_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_info(db_path, schema_name, table_name):\n",
    "    \"\"\"Function to use within build_schema_info to pull the column info under the specific table_name\"\"\"\n",
    "    \n",
    "    db = sqlite3.connect(db_path)\n",
    "    cursor=db.cursor()\n",
    "    cursor.execute(\"PRAGMA table_info(\" + table_name + \");\")\n",
    "    columns = cursor.fetchall()\n",
    "\n",
    "    column_data = (schema_name, table_name, columns)\n",
    "\n",
    "    cursor.close()\n",
    "    db.close()\n",
    "\n",
    "    return column_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the processing and only leave the additional columns created\n",
    "def df_text_processing(df, text_columns=['schema', 'table', 'c_name']):\n",
    "    \"\"\"Function to apply some simple text processing to our schema info\"\"\"\n",
    "    #df[text_columns] = df[text_columns].apply(lambda x: x.str.lower()) #make lowercase\n",
    "    \n",
    "    #punc = string.punctuation\n",
    "    #df[text_columns] = df[text_columns].apply(lambda x: [word for word in x if word not in punc]) #remove punctuation\n",
    "\n",
    "    for col in text_columns:\n",
    "        df[col+'_split'] = df[col].str.replace('_', ' ') #loop through gext columns and create versions that breakout the '_' connected titles into seperate words\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_schema_info(filepath, filetype):\n",
    "    \"\"\"Function combines the get_filenames, get_table_names, and get_column_info funciton to create list of file names in a subdirectory and running the PRAGMA table info against each, saving their results to a list.\n",
    "    It then builds a pandas dataframe with the full schema info\"\"\"\n",
    "    \n",
    "    schema_list = get_filenames(filepath, filetype)\n",
    "\n",
    "    schema_data = []\n",
    "\n",
    "    for schema in schema_list:\n",
    "        schema_name = schema\n",
    "        db_path = filepath + str(schema) + filetype\n",
    "\n",
    "        table_list = get_table_names(db_path)\n",
    "        for table in table_list:\n",
    "            table_name = table\n",
    "            column_data = get_column_info(db_path, schema_name, table_name)\n",
    "            schema_data.append(column_data)\n",
    "    \n",
    "    schema_df = (pd.DataFrame(schema_data, columns=['schema','table','column_info']).explode('column_info', ignore_index=True))\n",
    "\n",
    "    schema_df[['c_id','c_name','c_type','notnull','dflt_value','is_pk']] = schema_df.column_info.tolist()\n",
    "\n",
    "    schema_df.drop(columns=['column_info','notnull','dflt_value','is_pk'], inplace=True)\n",
    "\n",
    "    schema_df_processed = df_text_processing(schema_df)\n",
    "    \n",
    "    return schema_df_processed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_info = build_schema_info('../data/processed/db/', '.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>schema</th>\n",
       "      <th>table</th>\n",
       "      <th>c_id</th>\n",
       "      <th>c_name</th>\n",
       "      <th>c_type</th>\n",
       "      <th>schema_split</th>\n",
       "      <th>table_split</th>\n",
       "      <th>c_name_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coffee_shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>0</td>\n",
       "      <td>Shop_ID</td>\n",
       "      <td>INT</td>\n",
       "      <td>coffee shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>Shop ID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coffee_shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>1</td>\n",
       "      <td>Address</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>coffee shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>Address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coffee_shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>2</td>\n",
       "      <td>Num_of_staff</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>coffee shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>Num of staff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coffee_shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>3</td>\n",
       "      <td>Score</td>\n",
       "      <td>REAL</td>\n",
       "      <td>coffee shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>Score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coffee_shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>4</td>\n",
       "      <td>Open_Year</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>coffee shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>Open Year</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        schema table c_id        c_name c_type schema_split table_split  \\\n",
       "0  coffee_shop  shop    0       Shop_ID    INT  coffee shop        shop   \n",
       "1  coffee_shop  shop    1       Address   TEXT  coffee shop        shop   \n",
       "2  coffee_shop  shop    2  Num_of_staff   TEXT  coffee shop        shop   \n",
       "3  coffee_shop  shop    3         Score   REAL  coffee shop        shop   \n",
       "4  coffee_shop  shop    4     Open_Year   TEXT  coffee shop        shop   \n",
       "\n",
       "   c_name_split  \n",
       "0       Shop ID  \n",
       "1       Address  \n",
       "2  Num of staff  \n",
       "3         Score  \n",
       "4     Open Year  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_info.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Schema Info Columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parallel json file\n",
    "I can see some easier use with some of our future langchain steps with a json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dict of each schema-table combo with corresponding column information.\n",
    "schema_json = (schema_info.groupby(['schema', 'schema_split', 'table','table_split']) #breakout each of these columns into a record for schema-table combos\n",
    "       .apply(lambda x: x[['c_id','c_name','c_name_split','c_type']].to_dict('records')) #brekout these into dictionaries under each of the schema-table combos\n",
    "       .reset_index()\n",
    "       .rename(columns={0:'columns'})\n",
    "       .to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'schema': 'academic',\n",
       " 'schema_split': 'academic',\n",
       " 'table': 'author',\n",
       " 'table_split': 'author',\n",
       " 'columns': [{'c_id': 0,\n",
       "   'c_name': 'aid',\n",
       "   'c_name_split': 'aid',\n",
       "   'c_type': 'INT'},\n",
       "  {'c_id': 1,\n",
       "   'c_name': 'homepage',\n",
       "   'c_name_split': 'homepage',\n",
       "   'c_type': 'TEXT'},\n",
       "  {'c_id': 2, 'c_name': 'name', 'c_name_split': 'name', 'c_type': 'TEXT'},\n",
       "  {'c_id': 3, 'c_name': 'oid', 'c_name_split': 'oid', 'c_type': 'INT'}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to a json file - savings in our interim data folder\n",
    "with open('../data/interim/schema_info.json', 'w') as file:\n",
    "    json.dump(schema_json, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataframe to .pkl file\n",
    "\n",
    "I already saved to a json, but this will be incase I want to pull in the direct dataframe via a pickle file incase that's easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as pickle file\n",
    "filepath = '../data/interim/schema_info.pkl'\n",
    "schema_info.to_pickle(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

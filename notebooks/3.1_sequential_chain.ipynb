{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLDatabaseSequentialChain\n",
    "\n",
    "Using the simple chain, I got the process working, but the results aren't great. If I point the input variables to very specific places it can get the right answer. But using the standard prompt template through langchain doesn't give the correct results for our test queries. This prompt feeds in the table creation statements and first few rows of each table. In doing this it then relies on the AI to take the useful information based on the input question. I think there is some further prompt engineering I could do, but first I want to explore the other SQL options from Lanchain and iterate based on what I feel offers the best long-term solultions.\n",
    "\n",
    "This is this sequential chain that 1. Determines which tables to us based on the query. 2. Based on those tables, call the normal SQL database chain.\n",
    "\n",
    "This sounds similar, but I'm curious if parsing these and asking the model to do this in 2 steps will improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2023-08-01T00:52:32.569591-07:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.4\n",
      "IPython version      : 8.14.0\n",
      "\n",
      "Compiler    : Clang 15.0.7 \n",
      "OS          : Darwin\n",
      "Release     : 22.5.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from watermark import watermark\n",
    "print(watermark())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "from langchain import SQLDatabase, SQLDatabaseChain\n",
    "from langchain.chains import SQLDatabaseSequentialChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy Functions - Point to Database\n",
    "\n",
    "I could build out the whole automation, but I just want to test this chain so I will point it to the right place and do a test on our simplest quesiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_select(persist_dir, query):\n",
    "    \"\"\"\n",
    "    Load the schema info vector database from disk and run the input question against it to return the most likely database we need to pull data from.\n",
    "    \"\"\"\n",
    "    #setup embeddings using HuggingFace and the directory location\n",
    "    embeddings  = HuggingFaceEmbeddings()\n",
    "    per_dir = persist_dir\n",
    "\n",
    "    # load from disk\n",
    "    vectordb = Chroma(persist_directory=per_dir, embedding_function=embeddings)\n",
    "\n",
    "    #run prompt as query and get most likely results\n",
    "    result = vectordb.similarity_search(query, k=1)\n",
    "\n",
    "    #save variables\n",
    "    top_schema = result[0].metadata['schema']\n",
    "    top_table = result[0].metadata['table']\n",
    "    table_cols = result[0].metadata['columns']\n",
    "\n",
    "    top_result = (top_schema, top_table, table_cols)\n",
    "\n",
    "    return top_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_and_connect_db(filepath, filename):\n",
    "    \"\"\"Locate the absolute path of the given SQLITE database and connect to it via the langchain SQLDatabase.from_uri method.\n",
    "    filepath is the filepath within the repo, example '../data/db/data.db'\n",
    "    filename is just the filename.filetype, example 'data.db'\n",
    "    \"\"\"\n",
    "    base_dir = os.path.dirname(os.path.abspath(filepath+filename)) #get the full path within the device\n",
    "    db_path = os.path.join(base_dir, filename) #combine with filename to get db_path\n",
    "    db = SQLDatabase.from_uri(\"sqlite:///\" + db_path) #connect via the lanchain method\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm_model(env_variable, repo_id='tiiuae/falcon-7b-instruct', temp=0.5, max_length=200):\n",
    "    \"\"\"\n",
    "    Take in the target hugging face repo and the api key to setup the llm to use in our query chain\n",
    "    env_variable is the name of the variable that stores your API key in your .env file\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    hf_api_token = os.getenv(env_variable)\n",
    "    llm = HuggingFaceHub(repo_id=repo_id, huggingfacehub_api_token=hf_api_token, model_kwargs={\"temperature\": temp, \"max_length\": max_length})\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Chain\n",
    "\n",
    "This time use the SQLDatabaseSequentialChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sql_chain(llm, db, verbose=True):\n",
    "    \"\"\"Take in prompt template and input variables, along with the llm and db created from other functions to create our SQL Chain\"\"\"\n",
    "    db_chain = SQLDatabaseSequentialChain.from_llm(llm, db, verbose=verbose, use_query_checker=True) #create our database chain\n",
    "    \n",
    "    return db_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link Full Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_analyst_seq(question, vector_db_path='../data/processed/chromadb/schema-table-split', db_root_path='../data/processed/db/', sql_dialect='sqlite', env_api_key_var='hf_token'):\n",
    "    \"\"\"take in user info on where the databases and the api key are located and answer their question using the sql chain.\"\"\"\n",
    "    \n",
    "    #query our vector database with the question to get the schema most related to the question.\n",
    "    top_result = db_select(persist_dir=vector_db_path, query=question)\n",
    "    top_schema = top_result[0]\n",
    "    top_table = top_result[1]\n",
    "\n",
    "    #use this result to establish our database\n",
    "    db = locate_and_connect_db(filepath=db_root_path, filename=top_schema+'.'+sql_dialect)\n",
    "\n",
    "    #initialize our large language model - needs an API key - we'll use the standard variables\n",
    "    llm = load_llm_model(env_variable=env_api_key_var)\n",
    "\n",
    "    # #create prompt template using our preset template\n",
    "    # prompt_template = create_prompt_template(prompt_template=template)\n",
    "\n",
    "    #setup the sql chain using the standard variables\n",
    "    sql_chain = create_sql_chain(db=db, llm=llm)\n",
    "\n",
    "    #run sql_chain on question\n",
    "    print(db)\n",
    "    print(top_schema+'-'+top_table)\n",
    "    sql_chain(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain.sql_database.SQLDatabase object at 0x18d894950>\n",
      "department_management-head\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brettly/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table names to use:\n",
      "\u001b[33;1m\u001b[1;3m['head', 'management']\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Return the first row of the 'head' table in the 'department_management' schema.\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3m3 rows from 'head' table:\n",
      "head_ID\tname\tborn_\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sql_analyst_seq(\"Return the first row of the 'head' table in the 'department_management' schema.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is giving me similar issues of not pulling in the right tables. The difference here is that the first step identifies the tables for the model to review. If that errors it pulls in nothing and it appears the model is just winging it from that point. And even sometimes when it does pull in the tables to try to use, it still has trouble with the question. It's also harder to test this because the prompt template for the first step in the chain isn't available to view on chainlit. So I can't copy and paste it into chatgpt for a reference point.\n",
    "\n",
    "[Chainlit](https://docs.chainlit.io/overview) is a really cool tool that lets you interact with your application in a chat interface and view the intermediate steps the LLM is performing. \n",
    "\n",
    "In this case, I think the step to identify the right table(s) will be key in a production app, but this just wasn't working for me. So one more \"out-of-the-box\" langchain feature to try."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

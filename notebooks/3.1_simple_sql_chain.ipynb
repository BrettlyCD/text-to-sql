{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates\n",
    "\n",
    "Now that I have my vector database setup, I want to pull down the most likely schema and table and save the metadata as variables. Then I think the best way to proceed to is to create a prompt template that accepts the variables and allows a consistent entry point to the model.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "from langchain import SQLDatabase, SQLDatabaseChain\n",
    "from langchain.chains import SQLDatabaseSequentialChain\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Top Results from VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup embeddings using HuggingFace and the directory location\n",
    "embeddings  = HuggingFaceEmbeddings()\n",
    "persist_dir = '../data/processed/chromadb/schema-table-split'\n",
    "\n",
    "# load from disk\n",
    "vectordb = Chroma(persist_directory=persist_dir, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run prompt as query and get most likely results\n",
    "query = \"How many heads of the departments are older than 56?\" #first prompt from the training data\n",
    "\n",
    "result = vectordb.similarity_search(query, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Metadata to Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_schema = result[0].metadata['schema']\n",
    "top_table = result[0].metadata['table']\n",
    "table_cols = result[0].metadata['columns']\n",
    "\n",
    "print(top_schema, top_table, table_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build SQL Agent Through HF Hub\n",
    "\n",
    "The docs on langchain are focused around either accessing the OpenAI API or a local model. I'm hoping I can mix both and access a Hugging Face model through an API without having to store it locally. I think I can do this through the Hugging Face Hub, by combined the steps in different parts of the documentation:\n",
    "- [Langchain - Hugging Face Hub](https://python.langchain.com/docs/modules/model_io/models/llms/integrations/huggingface_hub)\n",
    "- [Langchain - SQL Agents](https://python.langchain.com/docs/modules/chains/popular/sqlite)\n",
    "\n",
    "I do have the start of the locally stored method commented out below.\n",
    "\n",
    "### Load API Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_api_token = os.getenv('hf_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add path to HF repo\n",
    "repo_id = 'tiiuae/falcon-7b-instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point to LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish llm model\n",
    "llm = HuggingFaceHub(repo_id=repo_id, huggingfacehub_api_token=hf_api_token, model_kwargs={\"temperature\": 0.5, \"max_length\": 128})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Simple SQL Agent\n",
    "Langchain has some awesome features, but I'll start with a simple chain that points to one table.\n",
    "\n",
    "First, I need to use the output of my vector db query to point to the sqlite database we want to work with.\n",
    "\n",
    "#### Point to sqlite db location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.dirname(os.path.abspath('../data/processed/db/department_management.sqlite'))\n",
    "db_path = os.path.join(BASE_DIR, \"department_management.sqlite\")\n",
    "\n",
    "db_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(\"sqlite:///\" + db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Prompt Template\n",
    "I'll even specify the single table here to really spoon feed the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_agent_prompt_template = \"\"\"You are an expert data analyst. Given an input question, first write a syntactically correct {dialect} query to run, then look at the results of the query and return and describe the answer.\n",
    "Use the following format:\n",
    "\n",
    "Question: Question here\n",
    "SQLQuery: SQL Query to run\n",
    "SQLResult: Result of the SQLQuery\n",
    "Answer: Final answer here\n",
    "\n",
    "Only use the following tables:\n",
    "{table_info}\n",
    "\n",
    "Question: {input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"table_info\", \"dialect\"], template=sql_agent_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup db_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain.from_llm(llm, db, prompt=sql_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with our simple question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain.run(sql_prompt.format(input=\"How many heads of the departments are older than 56?\", table_info=\"head\", dialect=\"sqlite\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "This simple chain method is working, although not as cleanly as I would like. The chain is inconsistent in returning \"5\" as the answer vs describing it with something like \"There are 5 department heads older than 56.\"\n",
    "\n",
    "I'm also struggling with the prompt template. I'm thinking there must be some preset formats in the langchain sql chain because anytime I try to change anything structurally, it errors out. So unforuntately this means the output usually includes \"Question:\" at the end. I'll need to check the documentation and online discussions a bit more to see if I can get around that.\n",
    "\n",
    "## Create Functions\n",
    "Even if I want some cleanup, this technically works which is great. But it is very segmented. So I want to pull the steps together so I don't have to manually change variables in each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_select(persist_dir, query):\n",
    "    \"\"\"\n",
    "    Load the schema info vector database from disk and run the input question against it to return the most likely database we need to pull data from.\n",
    "    \"\"\"\n",
    "    #setup embeddings using HuggingFace and the directory location\n",
    "    embeddings  = HuggingFaceEmbeddings()\n",
    "    per_dir = persist_dir\n",
    "\n",
    "    # load from disk\n",
    "    vectordb = Chroma(persist_directory=per_dir, embedding_function=embeddings)\n",
    "\n",
    "    #run prompt as query and get most likely results\n",
    "    result = vectordb.similarity_search(query, k=1)\n",
    "\n",
    "    #save variables\n",
    "    top_schema = result[0].metadata['schema']\n",
    "    top_table = result[0].metadata['table']\n",
    "    table_cols = result[0].metadata['columns']\n",
    "\n",
    "    top_result = (top_schema, top_table, table_cols)\n",
    "\n",
    "    return top_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_result = db_select(persist_dir='../data/processed/chromadb/schema-table-split', query='How many dpeartment heads are older than 56?')\n",
    "top_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_and_connect_db(filepath, filename):\n",
    "    \"\"\"Locate the absolute path of the given SQLITE database and connect to it via the langchain SQLDatabase.from_uri method.\n",
    "    filepath is the filepath within the repo, example '../data/db/data.db'\n",
    "    filename is just the filename.filetype, example 'data.db'\n",
    "    \"\"\"\n",
    "    base_dir = os.path.dirname(os.path.abspath(filepath+filename)) #get the full path within the device\n",
    "    db_path = os.path.join(base_dir, filename) #combine with filename to get db_path\n",
    "    db = SQLDatabase.from_uri(\"sqlite:///\" + db_path) #connect via the lanchain method\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm_model(env_variable, repo_id='tiiuae/falcon-7b-instruct', temp=0.5, max_length=64):\n",
    "    \"\"\"\n",
    "    Take in the target hugging face repo and the api key to setup the llm to use in our query chain\n",
    "    env_variable is the name of the variable that stores your API key in your .env file\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    hf_api_token = os.getenv(env_variable)\n",
    "    llm = HuggingFaceHub(repo_id=repo_id, huggingfacehub_api_token=hf_api_token, model_kwargs={\"temperature\": temp, \"max_length\": max_length})\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sql_chain(db, llm, verbose=True, input_vars=['input', 'table_info', 'dialect']\n",
    "                     , prompt_template=\n",
    "                        \"\"\"You are an expert data analyst. Given an input question, first write a syntactically correct {dialect} query to run, then look at the results of the query and return and describe the answer.\n",
    "Use the following format:\n",
    "\n",
    "Question: Question here\n",
    "SQLQuery: SQL Query to run\n",
    "SQLResult: Result of the SQLQuery\n",
    "Answer: Final answer here\n",
    "\n",
    "Only use the following tables:\n",
    "{table_info}\n",
    "\n",
    "Question: {input}\"\"\"\n",
    "                    ):\n",
    "    \"\"\"Take in prompt template and input variables, along with the llm and db created from other functions to create our SQL Chain\"\"\"\n",
    "    sql_prompt = PromptTemplate(input_variables=input_vars, template=prompt_template) #create our prompt template\n",
    "    db_chain = SQLDatabaseChain.from_llm(llm, db, prompt=sql_prompt, verbose=verbose) #create our database chain\n",
    "    \n",
    "    return db_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chain(db_chain, question, table, sql_dialect='sqlite'):\n",
    "    \"\"\"Function to run chain that will end our overall application function by taking in your question and variables created by the other functions\"\"\"\n",
    "    db_chain.run(sql_prompt.format(input=question, table_info=table, dialect=sql_dialect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_analyst(question, vector_db_path='../data/processed/chromadb/schema-table-split', db_root_path='../data/processed/db/', sql_dialect='sqlite', env_api_key_var='hf_token'):\n",
    "    \"\"\"take in user info on where the databases and the api key are located and answer their question using the sql chain.\"\"\"\n",
    "    \n",
    "    #query our vector database with the question to get the schema most related to the question.\n",
    "    top_result = db_select(persist_dir=vector_db_path, query=question)\n",
    "    top_schema = top_result[0]\n",
    "    top_table = top_result[1]\n",
    "\n",
    "    #use this result to establish our database\n",
    "    db = locate_and_connect_db(filepath=db_root_path, filename=top_schema+'.'+sql_dialect)\n",
    "\n",
    "    #initialize our large language model - needs an API key - we'll use the standard variables\n",
    "    llm = load_llm_model(env_variable=env_api_key_var)\n",
    "\n",
    "    #setup the sql chain using the standard variables\n",
    "    sql_chain = create_sql_chain(db=db, llm=llm)\n",
    "\n",
    "    #run sql_chain on question\n",
    "    run_chain(db_chain=sql_chain, question=question, table=top_table, sql_dialect=sql_dialect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "You are an expert data analyst. Given an input question, first write a syntactically correct sqlite query to run, then look at the results of the query and return and describe the answer.\n",
      "Use the following format:\n",
      "\n",
      "Question: Question here\n",
      "SQLQuery: SQL Query to run\n",
      "SQLResult: Result of the SQLQuery\n",
      "Answer: Final answer here\n",
      "\n",
      "Only use the following tables:\n",
      "head\n",
      "\n",
      "Question: How many heads of the departments are older than 56?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT COUNT(*) FROM head WHERE age > 56.\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(5,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3m5.\n",
      "\n",
      "The query returns a count of 5 heads of departments older than 56\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sql_analyst(\"How many heads of the departments are older than 56?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

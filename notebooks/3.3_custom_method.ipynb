{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Agent\n",
    "\n",
    "I had limited success with the built-in functions, so I'll test a custom flow, really a combination of tweaked langchain functions.\n",
    "\n",
    "I think part of this could be more specialized schema and table extraction, but for now I think I'll stick with my single schema example and try to expand later.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import HuggingFaceHub, SQLDatabase\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup query\n",
    "We'll use this to input into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Using the 'bike_1' schema, how many trips did not end in San Francisco?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Target Tables from Vectordb\n",
    "\n",
    "Load in the top 3 results and save just the unique schemas to a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup embeddings using HuggingFace and the directory location\n",
    "embeddings  = HuggingFaceEmbeddings()\n",
    "persist_dir = '../data/processed/chromadb/schema-table-split'\n",
    "\n",
    "# load from disk\n",
    "vectordb = Chroma(persist_directory=persist_dir, embedding_function=embeddings)\n",
    "\n",
    "#run similarity search\n",
    "top_results = vectordb.similarity_search(question, k=3) #just return the top for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Schema Table: bike 1 trip', metadata={'schema': 'bike_1', 'table': 'trip', 'columns': '[\"id\", \"duration\", \"start_date\", \"start_station_name\", \"start_station_id\", \"end_date\", \"end_station_name\", \"end_station_id\", \"bike_id\", \"subscription_type\", \"zip_code\"]'}),\n",
       " Document(page_content='Schema Table: bike 1 station', metadata={'schema': 'bike_1', 'table': 'station', 'columns': '[\"id\", \"name\", \"lat\", \"long\", \"dock_count\", \"city\", \"installation_date\"]'}),\n",
       " Document(page_content='Schema Table: bike 1 status', metadata={'schema': 'bike_1', 'table': 'status', 'columns': '[\"station_id\", \"bikes_available\", \"docks_available\", \"time\"]'})]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bike_1'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_results[0].metadata['schema']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Target Schemas\n",
    "\n",
    "Sets are unordered, lists aren't automatically unique. I want a unique set of values in order of how they are pulled from the document strings. In some tests, it's giving me the 2nd most relevant schema first. I need to find a way to make sure it always provides the schema names in the order provided by the similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bike_1']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try in a standard list with dict.fromkeys\n",
    "tgt_list = list(dict.fromkeys([doc.metadata['schema'] for doc in top_results]))\n",
    "\n",
    "tgt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bike_1'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_schemas = set(doc.metadata['schema'] for doc in top_results)\n",
    "\n",
    "target_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bike_1'}\n"
     ]
    }
   ],
   "source": [
    "target_schemas = set()\n",
    "for doc in top_results:\n",
    "    schema = doc.metadata['schema']\n",
    "    target_schemas.add(schema)\n",
    "\n",
    "print(target_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bike_1'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(target_schemas)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool - Query for Schema Information\n",
    "Use the langchain SQLDatabase class to get all the information about the schema. I think I also want to use the most likely columns returned to sort and prioritize how the tables are fed into the llm.\n",
    "\n",
    "I could load in the sql_query tools from langchain.tools, but I want to tweek things a little bit, especially in the order. I could come back to this later if it does simplify things and my way doesn't make the outcome any better.\n",
    "\n",
    "#### Point to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_filepath = '../data/processed/db/'\n",
    "db_filename = list(target_schemas)[0] + '.sqlite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point to database\n",
    "base_dir = os.path.dirname(os.path.abspath(db_filepath+db_filename)) #get the full path within the device\n",
    "db_path = os.path.join(base_dir, db_filename) #combine with filename to get db_path\n",
    "db = SQLDatabase.from_uri(\"sqlite:///\" + db_path) #connect via the lanchain method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set table sorting and then pull tables names using langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['status', 'trip', 'station']\n"
     ]
    }
   ],
   "source": [
    "#pull the table names from the vector query so we can have them in order of similarity to the question.\n",
    "table_sorting = set()\n",
    "for doc in top_results:\n",
    "    schema = doc.metadata['schema']\n",
    "    if schema == 'bike_1':\n",
    "        table_name = doc.metadata['table']\n",
    "        table_sorting.add(table_name)\n",
    "table_sorting = list(table_sorting)\n",
    "print(table_sorting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['status', 'trip', 'station', 'weather']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get tables names\n",
    "table_names = db.get_usable_table_names()\n",
    "\n",
    "#sort by the results of our vector query to load the most likely table in first. Not sure if this will make a difference, but can't imagine it would hurt.\n",
    "priority_tables = sorted(table_names, key=lambda x: (table_sorting.index(x) if x in table_sorting else float('inf'), table_names.index(x)))\n",
    "\n",
    "priority_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get SQL Dialect\n",
    "\n",
    "Using the langchain class again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_dialect = db.dialect\n",
    "\n",
    "sql_dialect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get table info\n",
    "\n",
    "You can do this for all tables in the database automatically with the function, but I want to continue trying to order these by likelihood of being related to the question. So I'll do this in a loop.\n",
    "\n",
    "This gives the full create statement and 3 sample rows, which I think will be a great start. And really gets us to where the Chain does for adding to the prompt template. I'll also combine this with the table name at the start of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDL for the status table:\n",
      "CREATE TABLE status (\n",
      "\tstation_id INTEGER, \n",
      "\tbikes_available INTEGER, \n",
      "\tdocks_available INTEGER, \n",
      "\ttime TEXT, \n",
      "\tFOREIGN KEY(station_id) REFERENCES station (id)\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from status table:\n",
      "station_id\tbikes_available\tdocks_available\ttime\n",
      "3\t12\t3\t2015-06-02 12:46:02\n",
      "3\t12\t3\t2015-06-02 12:47:02\n",
      "3\t12\t3\t2015-06-02 12:48:02\n",
      "*/\n",
      "\n",
      "DDL for the trip table:\n",
      "CREATE TABLE trip (\n",
      "\tid INTEGER, \n",
      "\tduration INTEGER, \n",
      "\tstart_date TEXT, \n",
      "\tstart_station_name TEXT, \n",
      "\tstart_station_id INTEGER, \n",
      "\tend_date TEXT, \n",
      "\tend_station_name TEXT, \n",
      "\tend_station_id INTEGER, \n",
      "\tbike_id INTEGER, \n",
      "\tsubscription_type TEXT, \n",
      "\tzip_code INTEGER, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from trip table:\n",
      "id\tduration\tstart_date\tstart_station_name\tstart_station_id\tend_date\tend_station_name\tend_station_id\tbike_id\tsubscription_type\tzip_code\n",
      "900504\t384\t8/21/2015 17:03\tHoward at 2nd\t63\t8/21/2015 17:10\tSan Francisco Caltrain 2 (330 Townsend)\t69\t454\tSubscriber\t94041\n",
      "900505\t588\t8/21/2015 17:03\tSouth Van Ness at Market\t66\t8/21/2015 17:13\tSan Francisco Caltrain 2 (330 Townsend)\t69\t574\tSubscriber\t95119\n",
      "900506\t196\t8/21/2015 17:04\tMarket at Sansome\t77\t8/21/2015 17:07\tHarry Bridges Plaza (Ferry Building)\t50\t636\tSubscriber\t94925\n",
      "*/\n",
      "\n",
      "DDL for the station table:\n",
      "CREATE TABLE station (\n",
      "\tid INTEGER, \n",
      "\tname TEXT, \n",
      "\tlat NUMERIC, \n",
      "\tlong NUMERIC, \n",
      "\tdock_count INTEGER, \n",
      "\tcity TEXT, \n",
      "\tinstallation_date TEXT, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from station table:\n",
      "id\tname\tlat\tlong\tdock_count\tcity\tinstallation_date\n",
      "2\tSan Jose Diridon Caltrain Station\t37.3297320000\t-121.9017820000\t27\tSan Jose\t8/6/2013\n",
      "3\tSan Jose Civic Center\t37.3306980000\t-121.8889790000\t15\tSan Jose\t8/5/2013\n",
      "4\tSanta Clara at Almaden\t37.3339880000\t-121.8949020000\t11\tSan Jose\t8/6/2013\n",
      "*/\n",
      "\n",
      "DDL for the weather table:\n",
      "CREATE TABLE weather (\n",
      "\tdate TEXT, \n",
      "\tmax_temperature_f INTEGER, \n",
      "\tmean_temperature_f INTEGER, \n",
      "\tmin_temperature_f INTEGER, \n",
      "\tmax_dew_point_f INTEGER, \n",
      "\tmean_dew_point_f INTEGER, \n",
      "\tmin_dew_point_f INTEGER, \n",
      "\tmax_humidity INTEGER, \n",
      "\tmean_humidity INTEGER, \n",
      "\tmin_humidity INTEGER, \n",
      "\tmax_sea_level_pressure_inches NUMERIC, \n",
      "\tmean_sea_level_pressure_inches NUMERIC, \n",
      "\tmin_sea_level_pressure_inches NUMERIC, \n",
      "\tmax_visibility_miles INTEGER, \n",
      "\tmean_visibility_miles INTEGER, \n",
      "\tmin_visibility_miles INTEGER, \n",
      "\t\"max_wind_Speed_mph\" INTEGER, \n",
      "\tmean_wind_speed_mph INTEGER, \n",
      "\tmax_gust_speed_mph INTEGER, \n",
      "\tprecipitation_inches INTEGER, \n",
      "\tcloud_cover INTEGER, \n",
      "\tevents TEXT, \n",
      "\twind_dir_degrees INTEGER, \n",
      "\tzip_code INTEGER\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from weather table:\n",
      "date\tmax_temperature_f\tmean_temperature_f\tmin_temperature_f\tmax_dew_point_f\tmean_dew_point_f\tmin_dew_point_f\tmax_humidity\tmean_humidity\tmin_humidity\tmax_sea_level_pressure_inches\tmean_sea_level_pressure_inches\tmin_sea_level_pressure_inches\tmax_visibility_miles\tmean_visibility_miles\tmin_visibility_miles\tmax_wind_Speed_mph\tmean_wind_speed_mph\tmax_gust_speed_mph\tprecipitation_inches\tcloud_cover\tevents\twind_dir_degrees\tzip_code\n",
      "8/29/2013\t74\t68\t61\t61\t58\t56\t93\t75\t57\t30.0700000000\t30.0200000000\t29.9700000000\t10\t10\t10\t23\t11\t28\t0\t4\t\t286\t94107\n",
      "8/30/2013\t78\t69\t60\t61\t58\t56\t90\t70\t50\t30.0500000000\t30.0000000000\t29.9300000000\t10\t10\t7\t29\t13\t35\t0\t2\t\t291\t94107\n",
      "8/31/2013\t71\t64\t57\t57\t56\t54\t93\t75\t57\t30.0000000000\t29.9600000000\t29.9200000000\t10\t10\t10\t26\t15\t31\t0\t4\t\t284\t94107\n",
      "*/\n"
     ]
    }
   ],
   "source": [
    "table_info = []\n",
    "for table in priority_tables:\n",
    "    table_name = table\n",
    "    info = db.get_table_info(table_names=[table])\n",
    "    table_info.append('DDL for the ' + table_name + ' table:' + info)\n",
    "\n",
    "#merge the items\n",
    "tables_summary = '\\n\\n'.join(table_info)\n",
    "\n",
    "print(tables_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_prompt = PromptTemplate(\n",
    "    input_variables=[],\n",
    "    template = f\"\"\"\n",
    "        You are a SQL Query Writer. Given an input question, first create a working {sql_dialect} SQL statement to find the answer to an input question and then return only the syntactically correct SQL statement.\n",
    "        \n",
    "        Use one or multiple of these tables:\n",
    "        {tables_summary} \n",
    "\n",
    "        Input question: \"{question}\"\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] output_parser=None partial_variables={} template='\\n        You are a SQL Query Writer. Given an input question, first create a working sqlite SQL statement to find the answer to an input question and then return only the syntactically correct SQL statement.\\n        \\n        Use one or multiple of these tables:\\n        DDL for the status table:\\nCREATE TABLE status (\\n\\tstation_id INTEGER, \\n\\tbikes_available INTEGER, \\n\\tdocks_available INTEGER, \\n\\ttime TEXT, \\n\\tFOREIGN KEY(station_id) REFERENCES station (id)\\n)\\n\\n/*\\n3 rows from status table:\\nstation_id\\tbikes_available\\tdocks_available\\ttime\\n3\\t12\\t3\\t2015-06-02 12:46:02\\n3\\t12\\t3\\t2015-06-02 12:47:02\\n3\\t12\\t3\\t2015-06-02 12:48:02\\n*/\\n\\nDDL for the trip table:\\nCREATE TABLE trip (\\n\\tid INTEGER, \\n\\tduration INTEGER, \\n\\tstart_date TEXT, \\n\\tstart_station_name TEXT, \\n\\tstart_station_id INTEGER, \\n\\tend_date TEXT, \\n\\tend_station_name TEXT, \\n\\tend_station_id INTEGER, \\n\\tbike_id INTEGER, \\n\\tsubscription_type TEXT, \\n\\tzip_code INTEGER, \\n\\tPRIMARY KEY (id)\\n)\\n\\n/*\\n3 rows from trip table:\\nid\\tduration\\tstart_date\\tstart_station_name\\tstart_station_id\\tend_date\\tend_station_name\\tend_station_id\\tbike_id\\tsubscription_type\\tzip_code\\n900504\\t384\\t8/21/2015 17:03\\tHoward at 2nd\\t63\\t8/21/2015 17:10\\tSan Francisco Caltrain 2 (330 Townsend)\\t69\\t454\\tSubscriber\\t94041\\n900505\\t588\\t8/21/2015 17:03\\tSouth Van Ness at Market\\t66\\t8/21/2015 17:13\\tSan Francisco Caltrain 2 (330 Townsend)\\t69\\t574\\tSubscriber\\t95119\\n900506\\t196\\t8/21/2015 17:04\\tMarket at Sansome\\t77\\t8/21/2015 17:07\\tHarry Bridges Plaza (Ferry Building)\\t50\\t636\\tSubscriber\\t94925\\n*/\\n\\nDDL for the station table:\\nCREATE TABLE station (\\n\\tid INTEGER, \\n\\tname TEXT, \\n\\tlat NUMERIC, \\n\\tlong NUMERIC, \\n\\tdock_count INTEGER, \\n\\tcity TEXT, \\n\\tinstallation_date TEXT, \\n\\tPRIMARY KEY (id)\\n)\\n\\n/*\\n3 rows from station table:\\nid\\tname\\tlat\\tlong\\tdock_count\\tcity\\tinstallation_date\\n2\\tSan Jose Diridon Caltrain Station\\t37.3297320000\\t-121.9017820000\\t27\\tSan Jose\\t8/6/2013\\n3\\tSan Jose Civic Center\\t37.3306980000\\t-121.8889790000\\t15\\tSan Jose\\t8/5/2013\\n4\\tSanta Clara at Almaden\\t37.3339880000\\t-121.8949020000\\t11\\tSan Jose\\t8/6/2013\\n*/\\n\\nDDL for the weather table:\\nCREATE TABLE weather (\\n\\tdate TEXT, \\n\\tmax_temperature_f INTEGER, \\n\\tmean_temperature_f INTEGER, \\n\\tmin_temperature_f INTEGER, \\n\\tmax_dew_point_f INTEGER, \\n\\tmean_dew_point_f INTEGER, \\n\\tmin_dew_point_f INTEGER, \\n\\tmax_humidity INTEGER, \\n\\tmean_humidity INTEGER, \\n\\tmin_humidity INTEGER, \\n\\tmax_sea_level_pressure_inches NUMERIC, \\n\\tmean_sea_level_pressure_inches NUMERIC, \\n\\tmin_sea_level_pressure_inches NUMERIC, \\n\\tmax_visibility_miles INTEGER, \\n\\tmean_visibility_miles INTEGER, \\n\\tmin_visibility_miles INTEGER, \\n\\t\"max_wind_Speed_mph\" INTEGER, \\n\\tmean_wind_speed_mph INTEGER, \\n\\tmax_gust_speed_mph INTEGER, \\n\\tprecipitation_inches INTEGER, \\n\\tcloud_cover INTEGER, \\n\\tevents TEXT, \\n\\twind_dir_degrees INTEGER, \\n\\tzip_code INTEGER\\n)\\n\\n/*\\n3 rows from weather table:\\ndate\\tmax_temperature_f\\tmean_temperature_f\\tmin_temperature_f\\tmax_dew_point_f\\tmean_dew_point_f\\tmin_dew_point_f\\tmax_humidity\\tmean_humidity\\tmin_humidity\\tmax_sea_level_pressure_inches\\tmean_sea_level_pressure_inches\\tmin_sea_level_pressure_inches\\tmax_visibility_miles\\tmean_visibility_miles\\tmin_visibility_miles\\tmax_wind_Speed_mph\\tmean_wind_speed_mph\\tmax_gust_speed_mph\\tprecipitation_inches\\tcloud_cover\\tevents\\twind_dir_degrees\\tzip_code\\n8/29/2013\\t74\\t68\\t61\\t61\\t58\\t56\\t93\\t75\\t57\\t30.0700000000\\t30.0200000000\\t29.9700000000\\t10\\t10\\t10\\t23\\t11\\t28\\t0\\t4\\t\\t286\\t94107\\n8/30/2013\\t78\\t69\\t60\\t61\\t58\\t56\\t90\\t70\\t50\\t30.0500000000\\t30.0000000000\\t29.9300000000\\t10\\t10\\t7\\t29\\t13\\t35\\t0\\t2\\t\\t291\\t94107\\n8/31/2013\\t71\\t64\\t57\\t57\\t56\\t54\\t93\\t75\\t57\\t30.0000000000\\t29.9600000000\\t29.9200000000\\t10\\t10\\t10\\t26\\t15\\t31\\t0\\t4\\t\\t284\\t94107\\n*/ \\n\\n        Input question: \"Using the \\'bike_1\\' schema, how many trips did not end in San Francisco?\"\\n    ' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "print(create_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup LLM\n",
    "I'll test the google flan-t5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get api key\n",
    "load_dotenv()\n",
    "hf_api_token = os.getenv('hf_token')\n",
    "\n",
    "#add path to HF repo\n",
    "repo_id = 'google/flan-t5-xxl'\n",
    "\n",
    "#establish llm model\n",
    "llm = HuggingFaceHub(repo_id=repo_id, huggingfacehub_api_token=hf_api_token, model_kwargs={\"temperature\": 0.1, \"max_length\": 512})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Chain - Generate SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference API: Input validation error: `inputs` must have less than 1024 tokens. Given: 1276",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m chain \u001b[39m=\u001b[39m LLMChain(llm\u001b[39m=\u001b[39mllm, prompt\u001b[39m=\u001b[39mcreate_prompt, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m sql_query \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39;49mpredict()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/chains/llm.py:252\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    238\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    168\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     inputs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/chains/llm.py:92\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     89\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     90\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 92\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/chains/llm.py:102\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    103\u001b[0m     prompts,\n\u001b[1;32m    104\u001b[0m     stop,\n\u001b[1;32m    105\u001b[0m     callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    106\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[1;32m    107\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/llms/base.py:139\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    133\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    137\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    138\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/llms/base.py:225\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    220\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    221\u001b[0m         )\n\u001b[1;32m    222\u001b[0m     run_managers \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    223\u001b[0m         dumpd(\u001b[39mself\u001b[39m), prompts, invocation_params\u001b[39m=\u001b[39mparams, options\u001b[39m=\u001b[39moptions\n\u001b[1;32m    224\u001b[0m     )\n\u001b[0;32m--> 225\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    226\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    227\u001b[0m     )\n\u001b[1;32m    228\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/llms/base.py:176\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    175\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 176\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    177\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    178\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/llms/base.py:163\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    155\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    160\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    161\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 163\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    164\u001b[0m                 prompts,\n\u001b[1;32m    165\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    166\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    167\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    168\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    169\u001b[0m             )\n\u001b[1;32m    170\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    172\u001b[0m         )\n\u001b[1;32m    173\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    174\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/llms/base.py:523\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    521\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[1;32m    522\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 523\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    524\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    525\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    526\u001b[0m     )\n\u001b[1;32m    527\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/text2sql/lib/python3.11/site-packages/langchain/llms/huggingface_hub.py:114\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient(inputs\u001b[39m=\u001b[39mprompt, params\u001b[39m=\u001b[39mparams)\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m response:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError raised by inference API: \u001b[39m\u001b[39m{\u001b[39;00mresponse[\u001b[39m'\u001b[39m\u001b[39merror\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mtask \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    116\u001b[0m     \u001b[39m# Text generation return includes the starter text.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     text \u001b[39m=\u001b[39m response[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39mlen\u001b[39m(prompt) :]\n",
      "\u001b[0;31mValueError\u001b[0m: Error raised by inference API: Input validation error: `inputs` must have less than 1024 tokens. Given: 1276"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=create_prompt, verbose=False)\n",
    "sql_query = chain.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Try-Except logic for flagging inputs with too many tokens.\n",
    "\n",
    "I think with this I can have the program print the error so the user knows what's happening but keep the loop moving so it doesn't totally break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error raised by inference API: Input validation error: `inputs` must have less than 1024 tokens. Given: 1276\n",
      "keeping it moving\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sql_query = chain.predict()\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    print('keeping it moving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT Count ( * ) FROM head'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain for checking SQL Query\n",
    "\n",
    "This is directly from the langchain check query prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_prompt = PromptTemplate(\n",
    "    input_variables=[],\n",
    "    template = f\"\"\"{sql_query}\n",
    "\n",
    "    Double check the {sql_dialect} query above for common mistakes, including:\n",
    "    - Using NOT IN with NULL values\n",
    "    - Using UNION when UNION ALL should have been used\n",
    "    - Using BETWEEN for exclusive ranges\n",
    "    - Data type mismatch in predicates\n",
    "    - Properly quoting identifiers\n",
    "    - Using the correct number of arguments for functions\n",
    "    - Casting to the correct data type\n",
    "    - Using the proper columns for joins\n",
    "\n",
    "    If there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=validate_prompt, verbose=False)\n",
    "validated_query = chain.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT count(*) FROM head'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validated_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run and Debug the Query\n",
    "\n",
    "Run the query on the database, if it is succesful, send the results to an LLM to interpret them.\n",
    "If unsuccessful, ask and LLM to debug and write a new one based on the error message. And then try again, only exiting the loop once it works.\n",
    "\n",
    "The wonderful folks at langchain created a run_no_throw funciton that returns the successful results or the error message. Not a full big error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[(10,)]'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_output = db.run_no_throw(validated_query)\n",
    "\n",
    "sql_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create prompt for answering the question\n",
    "analyze_prompt = PromptTemplate(\n",
    "    input_variables=[],\n",
    "    template = f\"\"\"\n",
    "    You are an expert data analyst. Given an output of an SQL query, first look at the output and then determine the answer to a user question.\n",
    "    \n",
    "    SQL Output: \"{sql_output}\"\n",
    "    Question: \"{question}\"\n",
    "\n",
    "    Describe your answer with one sentence:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test analyze chain\n",
    "chain = LLMChain(llm=llm, prompt=analyze_prompt, verbose=False)\n",
    "answer = chain.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are 10 rows in the 'head' table in the 'department_management\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a query that should error\n",
    "bad_query = 'SELECT count(a) FROM head'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Error: (sqlite3.OperationalError) no such column: a\\n[SQL: SELECT count(a) FROM head]\\n(Background on this error at: https://sqlalche.me/e/20/e3q8)'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_error = db.run_no_throw(bad_query)\n",
    "\n",
    "query_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create query that should return a null avlue\n",
    "empty_query = \"SELECT name FROM head WHERE head_id = '25'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[]'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_empty = db.run_no_throw(empty_query)\n",
    "\n",
    "test_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create prompt for debugging error\n",
    "debug_error_prompt = PromptTemplate(\n",
    "    input_variables=[],\n",
    "    template = f\"\"\"{bad_query}\n",
    "\n",
    "The query above produced the following error:\n",
    "\n",
    "{query_error}\n",
    "\n",
    "Rewrite the query with the error fixed:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create prompt for debugging error\n",
    "debug_empty_prompt = PromptTemplate(\n",
    "    input_variables=[],\n",
    "    template = f\"\"\"{empty_query}\n",
    "\n",
    "The query above produced no result. Try rewriting the query so it will return results to this question \"{question}\":\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSELECT count(a) FROM head\n",
      "\n",
      "The query above produced the following error:\n",
      "\n",
      "Error: (sqlite3.OperationalError) no such column: a\n",
      "[SQL: SELECT count(a) FROM head]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "\n",
      "Rewrite the query with the error fixed:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#test debug\n",
    "chain = LLMChain(llm=llm, prompt=debug_error_prompt, verbose=True)\n",
    "debugged = chain.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT count(*) FROM head'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debugged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSELECT name FROM head WHERE head_id = '25'\n",
      "\n",
      "The query above produced no result. Try rewriting the query so it will return results to this question \"How many rows in the 'head' table in the 'department_management' schema?\":\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#test empty debug\n",
    "chain = LLMChain(llm=llm, prompt=debug_empty_prompt, verbose=True)\n",
    "emp_debugged = chain.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT Count ( * ) FROM head'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_debugged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Functions\n",
    "\n",
    "### Selecting database, connecting to it, running all of the prompts, etc.\n",
    "\n",
    "I want to first build this out as a local application that has multiple entry point into the LLMs, and then work on building this out as a custom agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish unique chains\n",
    "create_chain = LLMChain(llm=llm, prompt=create_prompt, verbose=False)\n",
    "\n",
    "validate_chain = LLMChain(llm=llm, prompt=validate_prompt, verbose=False)\n",
    "\n",
    "analyze_chain = LLMChain(llm=llm, prompt=analyze_prompt, verbose=False)\n",
    "\n",
    "debug_error_chain = LLMChain(llm=llm, prompt=debug_error_prompt, verbose=False)\n",
    "\n",
    "debug_empty_chain = LLMChain(llm=llm, prompt=debug_empty_prompt, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_doc_search(question, vector_db=vectordb, k=3):\n",
    "    \"\"\"\n",
    "    Run similarity search through a vectordb query on the user input question and return the k most similar schema-table combinations in document form..\n",
    "    \"\"\"\n",
    "    similar_docs = vector_db.similarity_search(question, k)\n",
    "\n",
    "    return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_schemas(documents):\n",
    "    \"\"\"\n",
    "    Take in a list of documents created from our similar_doc_search function.\n",
    "    Use the metadata.\n",
    "    And convert this into a set of unique schemas returned by that search.\n",
    "    \"\"\"\n",
    "    target_schemas = set(doc.metadata['schema'] for doc in documents)\n",
    "\n",
    "    return target_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_db(db_path, target_schema):\n",
    "    \"\"\"\n",
    "    Take in the identified schema and connect to the sqlite database with that name\n",
    "    \"\"\"\n",
    "    db_filepath = db_path\n",
    "    db_filename = target_schema + '.sqlite'\n",
    "\n",
    "    #point to database\n",
    "    base_dir = os.path.dirname(os.path.abspath(db_filepath+db_filename)) #get the full path within the device\n",
    "    db_path = os.path.join(base_dir, db_filename) #combine with filename to get db_path\n",
    "    db = SQLDatabase.from_uri(\"sqlite:///\" + db_path) #connect via the lanchain method\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prioritize_tables(documents, target_schema, sql_database):\n",
    "    \"\"\"\n",
    "    Take in a list of similar_doc_search documents and prioritize tables from the same schema.\n",
    "    Sort all tables in the database based on the priority list.\n",
    "    \"\"\"\n",
    "    table_sorting = set(doc.metadata['table'] for doc in documents if doc.metadata['schema'] == target_schema)\n",
    "    table_names = sql_database.get_usable_table_names()\n",
    "    table_indices = {table: index for index, table in enumerate(table_sorting)}\n",
    "\n",
    "    priority_tables = sorted(table_names, key=lambda x: (table_indices.get(x, float('inf')), table_names.index(x)))\n",
    "\n",
    "    return priority_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_info(tables, database):\n",
    "    \"\"\"\n",
    "    Initialize an empty list, then loop through the list of tables in the database and save the DDL and 3 sample rows for each to a variable.\n",
    "    \"\"\"\n",
    "    table_info = []\n",
    "    for table in tables:\n",
    "        table_name = table\n",
    "        info = db.get_table_info(table_names=[table])\n",
    "        table_info.append('DDL for the ' + table_name + ' table:' + info)\n",
    "\n",
    "    #merge the items\n",
    "    tables_summary = '\\n\\n'.join(table_info)\n",
    "\n",
    "    return tables_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_dialect(database):\n",
    "    \"\"\"\n",
    "    Return the sql dialaect from the database.\n",
    "    \"\"\"\n",
    "    sql_dialect = database.dialect\n",
    "\n",
    "    return sql_dialect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_create_sql(sql_dialect, table_info, question, lang_model):\n",
    "    create_prompt = PromptTemplate(\n",
    "    input_variables=[],\n",
    "    template = f\"\"\"\n",
    "        You are a SQL Query Writer. Given an input question, first create a working {sql_dialect} SQL statement to find the answer to an input question and then return only the syntactically correct SQL statement.\n",
    "        \n",
    "        Use one or multiple of these tables:\n",
    "        {table_info} \n",
    "\n",
    "        Input question: \"{question}\"\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    create_chain = LLMChain(llm=lang_model, prompt=create_prompt, verbose=False)\n",
    "\n",
    "    sql_query = create_chain.predict()\n",
    "\n",
    "    return sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_check_sql(sql_query, sql_dialect, lang_model):\n",
    "    validate_prompt = PromptTemplate(\n",
    "    input_variables=[],\n",
    "    template = f\"\"\"{sql_query}\n",
    "\n",
    "        Double check the {sql_dialect} query above for common mistakes, including:\n",
    "        - Using NOT IN with NULL values\n",
    "        - Using UNION when UNION ALL should have been used\n",
    "        - Using BETWEEN for exclusive ranges\n",
    "        - Data type mismatch in predicates\n",
    "        - Properly quoting identifiers\n",
    "        - Using the correct number of arguments for functions\n",
    "        - Casting to the correct data type\n",
    "        - Using the proper columns for joins\n",
    "\n",
    "        If there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\"\"\"\n",
    "    )\n",
    "\n",
    "    validate_chain = LLMChain(llm=lang_model, prompt=validate_prompt, verbose=False)\n",
    "\n",
    "    checked_sql = validate_chain.predict()\n",
    "\n",
    "    return checked_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql(database, sql_query):\n",
    "    query_result = database.run_no_throw(sql_query)\n",
    "\n",
    "    return query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_debug_error(sql_query, error_message, lang_model):\n",
    "    \"\"\"\n",
    "    To be executed if the SQL query returns an error message\n",
    "    Will provide the error message to the LLM and ask it to debug, returning a new query\n",
    "    \"\"\"\n",
    "    #setup the debugging prompt\n",
    "    debug_error_prompt = PromptTemplate(\n",
    "        input_variables=[],\n",
    "        template = f\"\"\"{sql_query}\n",
    "\n",
    "    The query above produced the following error:\n",
    "\n",
    "    {error_message}\n",
    "\n",
    "    Rewrite the query with the error fixed:\"\"\"\n",
    "    )\n",
    "\n",
    "    debug_error_chain = LLMChain(llm=lang_model, prompt=debug_error_prompt, verbose=False)\n",
    "\n",
    "    debugged_query = debug_error_chain.predict()\n",
    "\n",
    "    return debugged_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_debug_empty(sql_query, question, lang_model):\n",
    "    \"\"\"\n",
    "    To be executed if the SQL query returns an empty string.\n",
    "    Will provide a prompt to the LLM asking it to re-review the original question and data and write a new query\n",
    "    \"\"\"\n",
    "    debug_empty_prompt = PromptTemplate(\n",
    "        input_variables=[],\n",
    "        template = f\"\"\"{sql_query}\n",
    "\n",
    "        The query above produced no result. Try rewriting the query so it will return results to this question \"{question}\":\"\"\"\n",
    "        )\n",
    "    \n",
    "    debug_empty_chain = LLMChain(llm=lang_model, prompt=debug_empty_prompt, verbose=False)\n",
    "\n",
    "    debugged_query = debug_empty_chain.predict()\n",
    "\n",
    "    return debugged_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_analyze(sql_query, query_result, question, lang_model):\n",
    "    \"\"\"\n",
    "    To be executed if the run SQL query returns a valid results.\n",
    "    Will provide the full output of the original question, final sql query, and answer.\n",
    "    \"\"\"\n",
    "    analyze_prompt = PromptTemplate(\n",
    "        input_variables=[],\n",
    "        template = f\"\"\"{query_result}\n",
    "\n",
    "        You are an expert data analyst. First look at the results of the above SQL output then identify the answer to this question:\n",
    "        Question: \"{question}\"\n",
    "\n",
    "        Then the answer in one sentence:\n",
    "        \"\"\"\n",
    "        )\n",
    "    \n",
    "    analyze_chain = LLMChain(llm=lang_model, prompt=analyze_prompt, verbose=False)\n",
    "    \n",
    "    llm_answer = analyze_chain.predict()\n",
    "    \n",
    "    output = f\"\"\"\n",
    "            Input question: {question}\n",
    "            SQL Query: {sql_query}\n",
    "            Answer: {llm_answer}\"\"\"\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_and_run(max_attempts=3, sql_query=validated_query):\n",
    "    \"\"\"\n",
    "    Execute the validated SQL query and try `max_attempts` times.\n",
    "    If successful, return the query result.\n",
    "    If not, raise an exception or return an appropriate error message.\n",
    "    \"\"\"\n",
    "    attempt = 1\n",
    "    while attempt <= max_attempts:\n",
    "        query_result = db.run_no_throw(sql_query)\n",
    "\n",
    "        if query_result[:5] == 'Error':\n",
    "            if attempt == max_attempts:\n",
    "                output = f\"Unable to execute the SQL query after {max_attempts} attempts.\"\n",
    "                break\n",
    "\n",
    "            sql_query = debug_error_chain.predict()\n",
    "            attempt += 1\n",
    "            time.sleep(1)  # Add a delay of 1 second between attempts\n",
    "\n",
    "\n",
    "        elif query_result == '[]':\n",
    "            if attempt == max_attempts:\n",
    "                output = \"Query returned no results after multiple attempts.\"\n",
    "                break\n",
    "\n",
    "            sql_query = debug_empty_chain.predict()\n",
    "            attempt +=1\n",
    "            time.sleep(1)  # Add a delay of 1 second between attempts\n",
    "\n",
    "        else:\n",
    "            answer = analyze_chain.predict()\n",
    "            output = f\"\"\"\n",
    "            Input question: {question}\n",
    "            SQL Query: {sql_query}\n",
    "            Answer: {query_result}\"\"\"\n",
    "            break\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT count(*) FROM head'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validated_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Input question: How many rows in the 'head' table in the 'department_management' schema?\n",
      "            SQL Query: SELECT count(*) FROM head\n",
      "            Answer: 10\n"
     ]
    }
   ],
   "source": [
    "output = debug_and_run()\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent\n",
    "\n",
    "https://python.langchain.com/docs/modules/agents/\n",
    "\n",
    "Two types ->\n",
    "- Action: at each step, decide on the next action using the outputs of the previous steps\n",
    "- Plan & Execute: decide on the full sequence up front, then execute them all without updating the plan\n",
    "\n",
    "Plan & Execute is better for larger problems because it maintains focus. Often it's best to combine and let P&E agents use action agents to execute plans.\n",
    "\n",
    "Tools -> actions an agent can take\n",
    "Toolkit -> wrapper around collections of tools for specifc use cases. Will need this for sql, tool to inspect tables and tool to run queries.\n",
    "\n",
    "#### Action Agents\n",
    "\n",
    "Wrapped in agent executors - call the agent, get back an action and action input, call the referenced tool, take the output of it and return it back to the agent.\n",
    "\n",
    "Typically has a prompt template, language model, and output parser\n",
    "\n",
    "#### Plan-and-Execute Agents\n",
    "\n",
    "Typically have the language model be the planner and the executor be an action agent.\n",
    "\n",
    "#### Built-In Agent Observations\n",
    "\n",
    "It appears the built-in model using an action agent - I'll try to tweak it. Maybe even by splitting out the steps into agents and trying the combo type with a larger plan & execute operating triggering the action agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType, load_tools, create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Toolkit\n",
    "\n",
    "I'll use the standard langchain one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Prompt Template - Create SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_prompt = PromptTemplate(\n",
    "    input_variables={\"tool_names\"},\n",
    "    template=\"\"\"\n",
    "        Use the following format:\n",
    "        Question: the input question you must answer\n",
    "        Thought: you should always think about what to do\n",
    "        Action: the action to take, should be one of [{tool_names}]\n",
    "        Action Input: the input to the action\n",
    "        Observation: the result of the action\n",
    "        Thought: I now know the final answer\n",
    "        Final Answer: SQL query ONLY, always include columns \"Indexes\", \"Index_Name\" along with your final answer. \n",
    "        If column name is a reserve word add quotes around it\n",
    "        \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_exec = create_sql_agent(llm=llm,\n",
    "                                toolkit=toolkit,\n",
    "                                verbose=True,\n",
    "                                format_instructions=create_prompt,\n",
    "                                top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: sql_db_query\n",
      "Action Input: \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: \n",
      "\n",
      "SELECT COUNT(*) FROM department WHERE age\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How many heads of the departments are older than 56?',\n",
       " 'output': 'SELECT COUNT(*) FROM department WHERE age'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_exec(\"How many heads of the departments are older than 56?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Prompt Template - Check the SQL Query\n",
    "\n",
    "From here: https://canvasapp.com/blog/text-to-sql-in-production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"sql_dialect\", \"current_day\", \"sql\"],\n",
    "    template=\"\"\"\n",
    "        You are a helpful AI that verifies that a SQL query runs correctly. If the query does not run successfully, you iteratively update the query based on the error message.\n",
    "        You follow the following procedure:\n",
    "            1. Run the input SQL query using the QueryTool\n",
    "            2. If this runs successfully, return immediately.\n",
    "            3. If the SQL query fails, debug the query:\n",
    "                3a. consider the error message\n",
    "                3b. update the SOL query\n",
    "                3c. try re-running\n",
    "            4. Repeat step 3 until you have a valid result. Finish and exit with the updated SQL query.\n",
    "        You are writing SQL for the {sql_dialect} dialect.\n",
    "        The current day is {current_day} in case the user references the date\n",
    "        The user's original question: {question}\n",
    "        The SQL: {sql}\n",
    "        Begin!\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "while True:\n",
    "    if i == 3:\n",
    "        output = 3\n",
    "        break\n",
    "    elif i < 7:\n",
    "        i += 1\n",
    "    else:\n",
    "        output = \"Didn't find that value, sorry!\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Didn't find that value, sorry!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
